{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import os, math\n",
    "import numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt, seaborn as sns\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "from pathlib import Path\n",
    "pd.set_option('display.max_columns', 1000)\n",
    "pd.set_option('display.max_rows', 400)\n",
    "sns.set()\n",
    "\n",
    "os.chdir('../..')\n",
    "from src import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA        = Path('data')\n",
    "RAW         = DATA/'raw'\n",
    "INTERIM     = DATA/'interim'\n",
    "PROCESSED   = DATA/'processed'\n",
    "SUBMISSIONS = DATA/'submissions'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import get_weeks, week_num\n",
    "week_labels = get_weeks(day_from=20160104, num_weeks=121)[104:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_feather(INTERIM/'metadata_train.feather')\n",
    "val = pd.read_feather(INTERIM/'metadata_val.feather')\n",
    "test = pd.read_feather(INTERIM/'metadata_val.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = ['Sector', 'Subsector', 'Region_x', 'Country', \n",
    "            'TickerIdx', 'Seniority', 'Currency', 'ActivityGroup', \n",
    "            'Region_y', 'Activity', 'RiskCaptain', 'Owner', \n",
    "            'IndustrySector', 'IndustrySubgroup', 'MarketIssue', 'CouponType']\n",
    "num_cols = ['ActualMaturityDateKey', 'IssueDateKey', 'CompositeRating', \n",
    "            'IssuedAmount', 'BondDuration']\n",
    "id_cols = ['CustomerIdx', 'IsinIdx', 'BuySell']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(INTERIM/'interest_sequences.pkl', 'rb') as f:\n",
    "    seq_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding cats...\n",
      "Scaling conts...\n",
      "Extracting seqs...\n",
      "CPU times: user 9.4 s, sys: 1.5 s, total: 10.9 s\n",
      "Wall time: 10.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from src.structured_lstm import preprocess\n",
    "scaler, train_seqs, val_seqs, test_seqs = preprocess(train, val, test, \n",
    "                                    cat_cols, num_cols, seq_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sandbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.structured_lstm import MultimodalDataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(MultimodalDataset(\n",
    "                train[cat_cols], train[num_cols],\n",
    "                train_seqs[:,:-2], train_seqs[:,-2]),\n",
    "                batch_size=128, shuffle=True)\n",
    "val_dl = DataLoader(MultimodalDataset(\n",
    "                val[cat_cols], val[num_cols],\n",
    "                val_seqs[:,:-1], val_seqs[:,-1]),\n",
    "                batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "from src.neuralnet import NeuralNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultimodalNet(nn.Module):\n",
    "    def __init__(self, emb_szs, n_cont, emb_drop, szs, drops, \n",
    "                 rnn_hidden_sz, rnn_input_sz, rnn_n_layers, rnn_drop,\n",
    "                 out_sz=1):\n",
    "        super().__init__()\n",
    "        self.structured_net = NeuralNet(emb_szs, n_cont=n_cont, \n",
    "                        emb_drop=emb_drop, szs=szs, drops=drops, \n",
    "                        out_sz=rnn_hidden_sz)\n",
    "        \n",
    "        self.lstm = nn.LSTM(rnn_input_sz, rnn_hidden_sz, rnn_n_layers, \n",
    "                            dropout=rnn_drop)\n",
    "        self.out = nn.Linear(rnn_hidden_sz, out_sz)\n",
    "        \n",
    "        self.rnn_n_layers = rnn_n_layers\n",
    "        self.rnn_hidden_sz = rnn_hidden_sz\n",
    "        \n",
    "    def forward(self, cats, conts, seqs, hidden):\n",
    "        x = self.structured_net(cats, conts) # [bs, hs]\n",
    "        cell = x.unsqueeze(0).expand(self.rnn_n_layers, *x.size()) # [nlay, bs, hs]\n",
    "        seqs = seqs.transpose(1,0).unsqueeze(2) # [sqlen, bs, 1] 1<=rnn_inp_sz\n",
    "        outputs, hidden = self.lstm(seqs, (hidden, cell.contiguous()))\n",
    "        out = self.out(outputs[-1]) # != if bidirectional\n",
    "        return out\n",
    "        \n",
    "    def init_hidden(self, batch_sz):\n",
    "        return torch.zeros(self.rnn_n_layers, batch_sz, self.rnn_hidden_sz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = next(iter(train_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([128, 16]),\n",
       " torch.Size([128, 6]),\n",
       " torch.Size([128, 14]),\n",
       " torch.Size([128]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0].shape, x[1].shape, x[2].shape, x[3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_szs = [int(train[col].max() + 1) for col in cat_cols]\n",
    "emb_szs = [(c, min(50, (c+1)//2)) for c in cat_szs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultimodalNet(emb_szs, n_cont=len(num_cols), emb_drop=0.04,\n",
    "                      szs=[1000,500], drops=[0.001, 0.01],\n",
    "                      rnn_hidden_sz=64, rnn_input_sz=1, rnn_n_layers=2,\n",
    "                      rnn_drop=0.04)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden = model.init_hidden(len(x[0]))\n",
    "output = model(x[0], x[1], x[2], hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model, cats, conts, seqs, hidden, \n",
    "               targets, optimizer, criterion):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    preds = model(cats, conts, seqs, hidden)\n",
    "    loss = criterion(preds.view(-1), targets)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(model, data_loader, print_every=800, USE_CUDA=False):\n",
    "    targets = []\n",
    "    preds = []\n",
    "    model.eval()\n",
    "    for batch_idx, (cats, conts, seqs, target) in enumerate(data_loader):\n",
    "        with torch.no_grad():            \n",
    "            hidden = model.init_hidden(len(cats))\n",
    "            if USE_CUDA:\n",
    "                cats, conts, target, hidden = cats.cuda(), conts.cuda(), \\\n",
    "                                              target.cuda(), hidden.cuda()\n",
    "            pred = model(cats, conts, seqs, hidden)\n",
    "            targets.extend(target.cpu())\n",
    "            preds.extend(pred.cpu())\n",
    "            assert len(targets) == len(preds)\n",
    "            if batch_idx % print_every == 0:\n",
    "                print('[{}/{} ({:.0f}%)]'.format(\n",
    "                        batch_idx * len(cats), len(data_loader.dataset),\n",
    "                        100. * batch_idx / len(data_loader)))\n",
    "    return [x.item() for x in targets], [F.sigmoid(x).item() for x in preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, optimizer, criterion,\n",
    "                n_epochs, print_every=200, val_every=5, USE_CUDA=False):\n",
    "    if USE_CUDA:\n",
    "        model = model.cuda()\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    val_auc_scores = []\n",
    "    val_every *= print_every\n",
    "    for epoch in range(n_epochs):\n",
    "        train_loss = 0\n",
    "        for batch_idx, (cats, conts, seqs, target) in enumerate(train_loader):\n",
    "            hidden = model.init_hidden(len(cats))\n",
    "            if USE_CUDA:\n",
    "                cats, conts, seqs, target = cats.cuda(), conts.cuda(), \\\n",
    "                                             seqs.cuda(), target.cuda()\n",
    "            train_loss += train_step(model, cats, conts, seqs, hidden, \n",
    "                                     target, optimizer, criterion)\n",
    "            \n",
    "            if batch_idx > 0 and batch_idx % print_every == 0:\n",
    "                train_loss /= print_every\n",
    "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                        epoch + 1, batch_idx * len(seqs), len(train_loader.dataset),\n",
    "                        100. * batch_idx / len(train_loader), train_loss))\n",
    "                train_losses.append(train_loss)\n",
    "                train_loss = 0\n",
    "            \n",
    "            if batch_idx > 0 and batch_idx % val_every == 0:\n",
    "                targets, preds = get_predictions(model, val_loader, USE_CUDA=USE_CUDA)\n",
    "                val_loss = nn.BCELoss()(torch.Tensor(preds),\n",
    "                                        torch.Tensor(targets)).item()\n",
    "                val_losses.append(val_loss)\n",
    "                val_auc = roc_auc_score(targets, preds)\n",
    "                val_auc_scores.append(val_auc)\n",
    "                print(f'ROC AUC Score: {val_auc:.6f}') \n",
    "                print(f'Validation Loss: {val_loss:.6f}')\n",
    "        print()\n",
    "    return model, train_losses, val_losses, val_auc_scores   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from torch import optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_CUDA = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultimodalNet(emb_szs, n_cont=len(num_cols), emb_drop=0.04,\n",
    "                      szs=[1000,500], drops=[0.001, 0.01],\n",
    "                      rnn_hidden_sz=64, rnn_input_sz=1, rnn_n_layers=2,\n",
    "                      rnn_drop=0.04)\n",
    "\n",
    "if USE_CUDA: model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(MultimodalDataset(\n",
    "                train[cat_cols], train[num_cols],\n",
    "                train_seqs[:,:-2], train_seqs[:,-2]),\n",
    "                batch_size=128, shuffle=True)\n",
    "val_dl = DataLoader(MultimodalDataset(\n",
    "                val[cat_cols], val[num_cols],\n",
    "                val_seqs[:,:-1], val_seqs[:,-1]),\n",
    "                batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model, train_losses, val_losses, val_auc_scores = train_model(\n",
    "                model, train_dl, val_dl, optimizer, criterion,\n",
    "                n_epochs=2, USE_CUDA=USE_CUDA, val_every=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch import optim\n",
    "import torch.nn as nn\n",
    "from src.structured_lstm import MultimodalDataset, MultimodalNet, train_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(MultimodalDataset(\n",
    "                train[cat_cols], train[num_cols],\n",
    "                train_seqs[:,:-2], train_seqs[:,-2]),\n",
    "                batch_size=128, shuffle=True)\n",
    "val_dl = DataLoader(MultimodalDataset(\n",
    "                val[cat_cols], val[num_cols],\n",
    "                val_seqs[:,:-1], val_seqs[:,-1]),\n",
    "                batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_szs = [int(train[col].max() + 1) for col in cat_cols]\n",
    "emb_szs = [(c, min(50, (c+1)//2)) for c in cat_szs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_CUDA = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultimodalNet(emb_szs, n_cont=len(num_cols), emb_drop=0.2,\n",
    "                      szs=[1000,500], drops=[0.5, 0.5],\n",
    "                      rnn_hidden_sz=64, rnn_input_sz=1, rnn_n_layers=2,\n",
    "                      rnn_drop=0.5)\n",
    "\n",
    "if USE_CUDA: model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [25600/983172 (3%)]\tLoss: 0.128011\n",
      "Train Epoch: 1 [51200/983172 (5%)]\tLoss: 0.087480\n",
      "Train Epoch: 1 [76800/983172 (8%)]\tLoss: 0.089640\n",
      "Train Epoch: 1 [102400/983172 (10%)]\tLoss: 0.095623\n",
      "Train Epoch: 1 [128000/983172 (13%)]\tLoss: 0.091929\n",
      "Train Epoch: 1 [153600/983172 (16%)]\tLoss: 0.095571\n",
      "Train Epoch: 1 [179200/983172 (18%)]\tLoss: 0.089275\n",
      "Train Epoch: 1 [204800/983172 (21%)]\tLoss: 0.089362\n",
      "Train Epoch: 1 [230400/983172 (23%)]\tLoss: 0.087040\n",
      "Train Epoch: 1 [256000/983172 (26%)]\tLoss: 0.079931\n",
      "[0/493590 (0%)]\n",
      "[102400/493590 (21%)]\n",
      "[204800/493590 (41%)]\n",
      "[307200/493590 (62%)]\n",
      "[409600/493590 (83%)]\n",
      "ROC AUC Score: 0.541212\n",
      "Validation Loss: 0.157820\n",
      "Train Epoch: 1 [281600/983172 (29%)]\tLoss: 0.084130\n",
      "Train Epoch: 1 [307200/983172 (31%)]\tLoss: 0.087296\n",
      "Train Epoch: 1 [332800/983172 (34%)]\tLoss: 0.077818\n",
      "Train Epoch: 1 [358400/983172 (36%)]\tLoss: 0.090080\n",
      "Train Epoch: 1 [384000/983172 (39%)]\tLoss: 0.079433\n",
      "Train Epoch: 1 [409600/983172 (42%)]\tLoss: 0.085986\n",
      "Train Epoch: 1 [435200/983172 (44%)]\tLoss: 0.085322\n",
      "Train Epoch: 1 [460800/983172 (47%)]\tLoss: 0.086526\n",
      "Train Epoch: 1 [486400/983172 (49%)]\tLoss: 0.085240\n",
      "Train Epoch: 1 [512000/983172 (52%)]\tLoss: 0.078193\n",
      "[0/493590 (0%)]\n",
      "[102400/493590 (21%)]\n",
      "[204800/493590 (41%)]\n",
      "[307200/493590 (62%)]\n",
      "[409600/493590 (83%)]\n",
      "ROC AUC Score: 0.562847\n",
      "Validation Loss: 0.160305\n",
      "Train Epoch: 1 [537600/983172 (55%)]\tLoss: 0.085442\n",
      "Train Epoch: 1 [563200/983172 (57%)]\tLoss: 0.086694\n",
      "Train Epoch: 1 [588800/983172 (60%)]\tLoss: 0.079975\n",
      "Train Epoch: 1 [614400/983172 (62%)]\tLoss: 0.083286\n",
      "Train Epoch: 1 [640000/983172 (65%)]\tLoss: 0.081614\n",
      "Train Epoch: 1 [665600/983172 (68%)]\tLoss: 0.085416\n",
      "Train Epoch: 1 [691200/983172 (70%)]\tLoss: 0.085841\n",
      "Train Epoch: 1 [716800/983172 (73%)]\tLoss: 0.085915\n",
      "Train Epoch: 1 [742400/983172 (76%)]\tLoss: 0.085305\n",
      "Train Epoch: 1 [768000/983172 (78%)]\tLoss: 0.086274\n",
      "[0/493590 (0%)]\n",
      "[102400/493590 (21%)]\n",
      "[204800/493590 (41%)]\n",
      "[307200/493590 (62%)]\n",
      "[409600/493590 (83%)]\n",
      "ROC AUC Score: 0.574533\n",
      "Validation Loss: 0.156474\n",
      "Train Epoch: 1 [793600/983172 (81%)]\tLoss: 0.089291\n",
      "Train Epoch: 1 [819200/983172 (83%)]\tLoss: 0.088176\n",
      "Train Epoch: 1 [844800/983172 (86%)]\tLoss: 0.081940\n",
      "Train Epoch: 1 [870400/983172 (89%)]\tLoss: 0.083242\n",
      "Train Epoch: 1 [896000/983172 (91%)]\tLoss: 0.082779\n",
      "Train Epoch: 1 [921600/983172 (94%)]\tLoss: 0.080271\n",
      "Train Epoch: 1 [947200/983172 (96%)]\tLoss: 0.081580\n",
      "Train Epoch: 1 [972800/983172 (99%)]\tLoss: 0.080803\n",
      "\n",
      "Train Epoch: 2 [25600/983172 (3%)]\tLoss: 0.084562\n",
      "Train Epoch: 2 [51200/983172 (5%)]\tLoss: 0.090258\n",
      "Train Epoch: 2 [76800/983172 (8%)]\tLoss: 0.082595\n",
      "Train Epoch: 2 [102400/983172 (10%)]\tLoss: 0.081251\n",
      "Train Epoch: 2 [128000/983172 (13%)]\tLoss: 0.081436\n",
      "Train Epoch: 2 [153600/983172 (16%)]\tLoss: 0.087990\n",
      "Train Epoch: 2 [179200/983172 (18%)]\tLoss: 0.086707\n",
      "Train Epoch: 2 [204800/983172 (21%)]\tLoss: 0.080579\n",
      "Train Epoch: 2 [230400/983172 (23%)]\tLoss: 0.086442\n",
      "Train Epoch: 2 [256000/983172 (26%)]\tLoss: 0.079922\n",
      "[0/493590 (0%)]\n",
      "[102400/493590 (21%)]\n",
      "[204800/493590 (41%)]\n",
      "[307200/493590 (62%)]\n",
      "[409600/493590 (83%)]\n",
      "ROC AUC Score: 0.585368\n",
      "Validation Loss: 0.157241\n",
      "Train Epoch: 2 [281600/983172 (29%)]\tLoss: 0.084234\n",
      "Train Epoch: 2 [307200/983172 (31%)]\tLoss: 0.084789\n",
      "Train Epoch: 2 [332800/983172 (34%)]\tLoss: 0.082519\n",
      "Train Epoch: 2 [358400/983172 (36%)]\tLoss: 0.081141\n",
      "Train Epoch: 2 [384000/983172 (39%)]\tLoss: 0.079565\n",
      "Train Epoch: 2 [409600/983172 (42%)]\tLoss: 0.079978\n",
      "Train Epoch: 2 [435200/983172 (44%)]\tLoss: 0.082281\n",
      "Train Epoch: 2 [460800/983172 (47%)]\tLoss: 0.084153\n",
      "Train Epoch: 2 [486400/983172 (49%)]\tLoss: 0.084041\n",
      "Train Epoch: 2 [512000/983172 (52%)]\tLoss: 0.085525\n",
      "[0/493590 (0%)]\n",
      "[102400/493590 (21%)]\n",
      "[204800/493590 (41%)]\n",
      "[307200/493590 (62%)]\n",
      "[409600/493590 (83%)]\n",
      "ROC AUC Score: 0.582974\n",
      "Validation Loss: 0.153366\n",
      "Train Epoch: 2 [537600/983172 (55%)]\tLoss: 0.082364\n",
      "Train Epoch: 2 [563200/983172 (57%)]\tLoss: 0.080691\n",
      "Train Epoch: 2 [588800/983172 (60%)]\tLoss: 0.085741\n",
      "Train Epoch: 2 [614400/983172 (62%)]\tLoss: 0.084355\n",
      "Train Epoch: 2 [640000/983172 (65%)]\tLoss: 0.082580\n",
      "Train Epoch: 2 [665600/983172 (68%)]\tLoss: 0.080326\n",
      "Train Epoch: 2 [691200/983172 (70%)]\tLoss: 0.079828\n",
      "Train Epoch: 2 [716800/983172 (73%)]\tLoss: 0.080002\n",
      "Train Epoch: 2 [742400/983172 (76%)]\tLoss: 0.083729\n",
      "Train Epoch: 2 [768000/983172 (78%)]\tLoss: 0.077698\n",
      "[0/493590 (0%)]\n",
      "[102400/493590 (21%)]\n",
      "[204800/493590 (41%)]\n",
      "[307200/493590 (62%)]\n",
      "[409600/493590 (83%)]\n",
      "ROC AUC Score: 0.589472\n",
      "Validation Loss: 0.158835\n",
      "Train Epoch: 2 [793600/983172 (81%)]\tLoss: 0.078975\n",
      "Train Epoch: 2 [819200/983172 (83%)]\tLoss: 0.082674\n",
      "Train Epoch: 2 [844800/983172 (86%)]\tLoss: 0.080329\n",
      "Train Epoch: 2 [870400/983172 (89%)]\tLoss: 0.084638\n",
      "Train Epoch: 2 [896000/983172 (91%)]\tLoss: 0.076600\n",
      "Train Epoch: 2 [921600/983172 (94%)]\tLoss: 0.091283\n",
      "Train Epoch: 2 [947200/983172 (96%)]\tLoss: 0.078355\n",
      "Train Epoch: 2 [972800/983172 (99%)]\tLoss: 0.079588\n",
      "\n",
      "CPU times: user 3min 22s, sys: 3.89 s, total: 3min 26s\n",
      "Wall time: 3min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model, train_losses, val_losses, val_auc_scores = train_model(\n",
    "                model, train_dl, val_dl, optimizer, criterion,\n",
    "                n_epochs=2, USE_CUDA=USE_CUDA, val_every=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
