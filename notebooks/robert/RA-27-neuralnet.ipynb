{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import os, math\n",
    "import numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt, seaborn as sns\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "from pathlib import Path\n",
    "pd.set_option('display.max_columns', 1000)\n",
    "pd.set_option('display.max_rows', 400)\n",
    "sns.set()\n",
    "\n",
    "os.chdir('../..')\n",
    "from src import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA        = Path('data')\n",
    "RAW         = DATA/'raw'\n",
    "INTERIM     = DATA/'interim'\n",
    "PROCESSED   = DATA/'processed'\n",
    "SUBMISSIONS = DATA/'submissions'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import get_weeks\n",
    "# week_labels = get_weeks(day_from=20160104, num_weeks=121)[52:]\n",
    "# week_labels = get_weeks(day_from=20160104, num_weeks=121)[96:]\n",
    "week_labels = get_weeks(day_from=20160104, num_weeks=121)[104:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20180101, 20180108, 20180115, 20180122, 20180129, 20180205, 20180212, 20180219, 20180226, 20180305, 20180312, 20180319, 20180326, 20180402, 20180409, 20180416, 20180423]\n"
     ]
    }
   ],
   "source": [
    "print(week_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 25s, sys: 45.8 s, total: 2min 11s\n",
      "Wall time: 2min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train = pd.DataFrame()\n",
    "for name in week_labels[:-2]:\n",
    "    train = pd.concat([train, pd.read_feather(PROCESSED/f'SVD_17-18_72f/week_{name}_SVD_diffscount.feather')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 636 ms, sys: 348 ms, total: 984 ms\n",
      "Wall time: 980 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "val = pd.read_feather(PROCESSED/f'SVD_17-18_72f/week_{week_labels[-2]}_SVD_diffscount.feather')\n",
    "test = pd.read_feather(PROCESSED/f'SVD_17-18_72f/week_{week_labels[-1]}_SVD_diffscount.feather')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fill nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_cols = ['Subsector', 'IndustrySector', 'IndustrySubgroup', 'MarketIssue']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.25 s, sys: 2.59 s, total: 8.84 s\n",
      "Wall time: 8.83 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train.fillna({x: '-999' for x in nan_cols}, inplace=True)\n",
    "val.fillna({x: '-999' for x in nan_cols}, inplace=True)\n",
    "test.fillna({x: '-999' for x in nan_cols}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = ['BuySell', 'Sector', 'Subsector', 'Region_x', 'Country', \n",
    "            'TickerIdx', 'Seniority', 'Currency', 'ActivityGroup', \n",
    "            'Region_y', 'Activity', 'RiskCaptain', 'Owner', \n",
    "            'IndustrySector', 'IndustrySubgroup', 'MarketIssue', 'CouponType']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_cols = ['TradeDateKey', 'CustomerIdx', 'IsinIdx']\n",
    "target_col = 'CustomerInterest'\n",
    "pred_col = 'PredictionIdx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = [c for c in train.columns \\\n",
    "              if c not in id_cols + cat_cols and c != target_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 23.8 s, sys: 5.41 s, total: 29.2 s\n",
      "Wall time: 29.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Label encode cats\n",
    "from src.utils import to_cat_codes, apply_cats\n",
    "to_cat_codes(train, cat_cols)\n",
    "apply_cats(val, train)\n",
    "apply_cats(test, train)\n",
    "\n",
    "for col in cat_cols:\n",
    "    train[col] = train[col].cat.codes\n",
    "    val[col] = val[col].cat.codes\n",
    "    test[col] = test[col].cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale conts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_szs = [train[col].nunique() for col in cat_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 5, 36, 3, 86, 3239, 9, 21, 3, 8, 15, 37, 101, 14, 330, 15, 6]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_szs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_szs = [(c, min(50, (c+1)//2)) for c in cat_szs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, 1),\n",
       " (5, 3),\n",
       " (36, 18),\n",
       " (3, 2),\n",
       " (86, 43),\n",
       " (3239, 50),\n",
       " (9, 5),\n",
       " (21, 11),\n",
       " (3, 2),\n",
       " (8, 4),\n",
       " (15, 8),\n",
       " (37, 19),\n",
       " (101, 50),\n",
       " (14, 7),\n",
       " (330, 50),\n",
       " (15, 8),\n",
       " (6, 3)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_szs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, emb_szs, n_cont, emb_drop, out_sz, szs, drops,\n",
    "                 use_bn=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embs = nn.ModuleList([\n",
    "            nn.Embedding(c, s) for c,s in emb_szs\n",
    "        ])\n",
    "        for emb in self.embs:\n",
    "            self.emb_init(emb)\n",
    "            \n",
    "        n_emb = sum(e.embedding_dim for e in self.embs)\n",
    "        self.n_emb, self.n_cont = n_emb, n_cont\n",
    "        szs = [n_emb + n_cont] + szs\n",
    "        \n",
    "        self.lins = nn.ModuleList([\n",
    "            nn.Linear(szs[i], szs[i+1]) for i in range(len(szs)-1)\n",
    "        ])\n",
    "        for o in self.lins: \n",
    "            nn.init.kaiming_normal_(o.weight.data)\n",
    "        \n",
    "        self.bns = nn.ModuleList([\n",
    "            nn.BatchNorm1d(sz) for sz in szs[1:]\n",
    "        ])        \n",
    "            \n",
    "        self.outp = nn.Linear(szs[-1], out_sz)\n",
    "        nn.init.kaiming_normal_(self.outp.weight.data)\n",
    "        \n",
    "        self.emb_drop = nn.Dropout(emb_drop)\n",
    "        self.drops = nn.ModuleList([\n",
    "            nn.Dropout(drop) for drop in drops\n",
    "        ])\n",
    "        self.bn = nn.BatchNorm1d(n_cont)\n",
    "        \n",
    "        self.use_bn = use_bn\n",
    "    \n",
    "    def forward(self, x_cat, x_cont):\n",
    "        if self.n_emb != 0:\n",
    "            x = [emb(x_cat[:,i]) for i,emb in enumerate(self.embs)]\n",
    "            x = torch.cat(x, 1)\n",
    "            x = self.emb_drop(x)\n",
    "        if self.n_cont != 0:\n",
    "            x2 = self.bn(x_cont)\n",
    "            x = torch.cat([x, x2], 1) if self.n_emb != 0 else x2\n",
    "        for lin, drop, bn in zip(self.lins, self.drops, self.bns):\n",
    "            x = F.relu(lin(x))\n",
    "            if self.use_bn:\n",
    "                x = bn(x)\n",
    "            x = drop(x)\n",
    "        return self.outp(x) # coupled with BCEWithLogitsLoss\n",
    "    \n",
    "    def emb_init(self, x):\n",
    "        # higher init range for low-dimensional embeddings\n",
    "        x = x.weight.data\n",
    "        sc = 2 / (x.size(1) + 1)\n",
    "        x.uniform_(-sc, sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabularDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, cat_cols, num_cols, target_col=None):\n",
    "        self.cats = df[cat_cols].values.astype(np.int64)\n",
    "        self.conts = df[num_cols].values.astype(np.float32)\n",
    "        self.target = df[target_col].values.astype(np.float32) if target_col \\\n",
    "                            else np.zeros((len(df),1)).astype(np.float32)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.target)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return [self.cats[idx], self.conts[idx], self.target[idx]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.72 s, sys: 2.71 s, total: 4.43 s\n",
      "Wall time: 4.43 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_ds = DataLoader(TabularDataset(train, cat_cols, num_cols, target_col), batch_size=128)\n",
    "val_ds = DataLoader(TabularDataset(val, cat_cols, num_cols, target_col), batch_size=128)\n",
    "test_ds = DataLoader(TabularDataset(val, cat_cols, num_cols), batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNet(emb_szs, n_cont=len(num_cols), emb_drop=0.04, \n",
    "                  out_sz=1, szs=[1000, 500], drops=[0.001, 0.01],\n",
    "                  use_bn=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, optimizer, train_loader, val_loader, print_every=100):\n",
    "    for epoch in range(2):\n",
    "        model.train()\n",
    "        train_loss, val_loss = 0, 0\n",
    "        for batch_idx, (cats, conts, target) in enumerate(train_loader):\n",
    "            cats, conts, target = Variable(cats), Variable(conts), \\\n",
    "                                  Variable(target)\n",
    "            pred = model(cats, conts)\n",
    "            loss = criterion(pred.view(-1), target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if batch_idx % print_every == 0:\n",
    "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                        epoch, batch_idx * len(cats), len(train_loader.dataset),\n",
    "                        100. * batch_idx / len(train_loader), loss.data[0]))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raduviri/anaconda3/envs/chana/lib/python3.6/site-packages/ipykernel_launcher.py:14: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/8102750 (0%)]\tLoss: 0.169503\n",
      "Train Epoch: 0 [12800/8102750 (0%)]\tLoss: 1.651523\n",
      "Train Epoch: 0 [25600/8102750 (0%)]\tLoss: 5.626127\n",
      "Train Epoch: 0 [38400/8102750 (0%)]\tLoss: 0.777354\n",
      "Train Epoch: 0 [51200/8102750 (1%)]\tLoss: 0.873584\n",
      "Train Epoch: 0 [64000/8102750 (1%)]\tLoss: 0.663155\n",
      "Train Epoch: 0 [76800/8102750 (1%)]\tLoss: 4.667749\n",
      "Train Epoch: 0 [89600/8102750 (1%)]\tLoss: 1.164287\n",
      "Train Epoch: 0 [102400/8102750 (1%)]\tLoss: 0.012571\n",
      "Train Epoch: 0 [115200/8102750 (1%)]\tLoss: 10.398273\n",
      "Train Epoch: 0 [128000/8102750 (2%)]\tLoss: 2.110725\n",
      "Train Epoch: 0 [140800/8102750 (2%)]\tLoss: 3.665831\n",
      "Train Epoch: 0 [153600/8102750 (2%)]\tLoss: 2.198929\n",
      "Train Epoch: 0 [166400/8102750 (2%)]\tLoss: 0.434258\n",
      "Train Epoch: 0 [179200/8102750 (2%)]\tLoss: 5.803332\n",
      "Train Epoch: 0 [192000/8102750 (2%)]\tLoss: 5.824392\n",
      "Train Epoch: 0 [204800/8102750 (3%)]\tLoss: 1.261248\n",
      "Train Epoch: 0 [217600/8102750 (3%)]\tLoss: 13.456893\n",
      "Train Epoch: 0 [230400/8102750 (3%)]\tLoss: 15.780764\n",
      "Train Epoch: 0 [243200/8102750 (3%)]\tLoss: 2.699002\n",
      "Train Epoch: 0 [256000/8102750 (3%)]\tLoss: 5.635906\n",
      "Train Epoch: 0 [268800/8102750 (3%)]\tLoss: 4.606927\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-101-762f1cb2e987>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_ds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-100-9e8cd526715d>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, optimizer, train_loader, val_loader, print_every)\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbatch_idx\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mprint_every\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m                 print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
      "\u001b[0;32m~/anaconda3/envs/chana/lib/python3.6/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     98\u001b[0m                     \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_exp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m                     \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m                 \u001b[0mbias_correction1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'step'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = train_model(model, optimizer, train_ds, val_ds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
