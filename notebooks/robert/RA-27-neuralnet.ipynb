{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import os, math\n",
    "import numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt, seaborn as sns\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "from pathlib import Path\n",
    "pd.set_option('display.max_columns', 1000)\n",
    "pd.set_option('display.max_rows', 400)\n",
    "sns.set()\n",
    "\n",
    "os.chdir('../..')\n",
    "from src import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA        = Path('data')\n",
    "RAW         = DATA/'raw'\n",
    "INTERIM     = DATA/'interim'\n",
    "PROCESSED   = DATA/'processed'\n",
    "SUBMISSIONS = DATA/'submissions'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import get_weeks\n",
    "# week_labels = get_weeks(day_from=20160104, num_weeks=121)[52:]\n",
    "# week_labels = get_weeks(day_from=20160104, num_weeks=121)[96:]\n",
    "week_labels = get_weeks(day_from=20160104, num_weeks=121)[104:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20180101, 20180108, 20180115, 20180122, 20180129, 20180205, 20180212, 20180219, 20180226, 20180305, 20180312, 20180319, 20180326, 20180402, 20180409, 20180416, 20180423]\n"
     ]
    }
   ],
   "source": [
    "print(week_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 23s, sys: 41.1 s, total: 2min 4s\n",
      "Wall time: 2min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train = pd.DataFrame()\n",
    "for name in week_labels[:-2]:\n",
    "    train = pd.concat([train, pd.read_feather(PROCESSED/f'SVD_17-18_72f/week_{name}_SVD_diffscount.feather')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 800 ms, sys: 568 ms, total: 1.37 s\n",
      "Wall time: 3.24 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "val = pd.read_feather(PROCESSED/f'SVD_17-18_72f/week_{week_labels[-2]}_SVD_diffscount.feather')\n",
    "test = pd.read_feather(PROCESSED/f'SVD_17-18_72f/week_{week_labels[-1]}_SVD_diffscount.feather')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fill nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "nan_cols = ['Subsector', 'IndustrySector', 'IndustrySubgroup', 'MarketIssue']\n",
    "train.fillna({x: '-999' for x in nan_cols}, inplace=True)\n",
    "val.fillna({x: '-999' for x in nan_cols}, inplace=True)\n",
    "test.fillna({x: '-999' for x in nan_cols}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = ['BuySell', 'Sector', 'Subsector', 'Region_x', 'Country', \n",
    "            'TickerIdx', 'Seniority', 'Currency', 'ActivityGroup', \n",
    "            'Region_y', 'Activity', 'RiskCaptain', 'Owner', \n",
    "            'IndustrySector', 'IndustrySubgroup', 'MarketIssue', 'CouponType']\n",
    "id_cols = ['TradeDateKey', 'CustomerIdx', 'IsinIdx']\n",
    "target_col = 'CustomerInterest'\n",
    "pred_col = 'PredictionIdx'\n",
    "num_cols = [c for c in train.columns \\\n",
    "              if c not in id_cols + cat_cols and c != target_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13.7 s, sys: 2.58 s, total: 16.3 s\n",
      "Wall time: 16.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Label encode cats\n",
    "from src.utils import to_cat_codes, apply_cats\n",
    "to_cat_codes(train, cat_cols)\n",
    "apply_cats(val, train)\n",
    "apply_cats(test, train)\n",
    "\n",
    "for col in cat_cols:\n",
    "    train[col] = train[col].cat.codes\n",
    "    val[col] = val[col].cat.codes\n",
    "    test[col] = test[col].cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BuySell 0 0 0\n",
      "Sector 0 0 0\n",
      "Subsector 0 0 0\n",
      "Region_x 0 0 0\n",
      "Country 0 0 0\n",
      "TickerIdx 0 -1 0\n",
      "Seniority 0 0 0\n",
      "Currency 0 0 0\n",
      "ActivityGroup 0 0 0\n",
      "Region_y 0 0 0\n",
      "Activity 0 0 0\n",
      "RiskCaptain 0 0 0\n",
      "Owner 0 0 0\n",
      "IndustrySector 0 0 0\n",
      "IndustrySubgroup 0 0 0\n",
      "MarketIssue 0 0 0\n",
      "CouponType 0 0 0\n"
     ]
    }
   ],
   "source": [
    "for col in cat_cols:\n",
    "    print(col, train[col].min(), val[col].min(), test[col].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Val has 1 OOF \n",
    "train['TickerIdx'] = train['TickerIdx'] + 1\n",
    "val['TickerIdx'] = val['TickerIdx'] + 1\n",
    "test['TickerIdx'] = test['TickerIdx'] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/chana/lib/python3.6/site-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, join_axes, ignore_index, keys, levels, names, verify_integrity, copy)\u001b[0m\n\u001b[1;32m    211\u001b[0m                        \u001b[0mverify_integrity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverify_integrity\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m                        copy=copy)\n\u001b[0;32m--> 213\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/chana/lib/python3.6/site-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36mget_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    406\u001b[0m             new_data = concatenate_block_managers(\n\u001b[1;32m    407\u001b[0m                 \u001b[0mmgrs_indexers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_axes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconcat_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 408\u001b[0;31m                 copy=self.copy)\n\u001b[0m\u001b[1;32m    409\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m                 \u001b[0mnew_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_consolidate_inplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/chana/lib/python3.6/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mconcatenate_block_managers\u001b[0;34m(mgrs_indexers, axes, concat_axis, copy)\u001b[0m\n\u001b[1;32m   5201\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5202\u001b[0m             b = make_block(\n\u001b[0;32m-> 5203\u001b[0;31m                 \u001b[0mconcatenate_join_units\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjoin_units\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconcat_axis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5204\u001b[0m                 placement=placement)\n\u001b[1;32m   5205\u001b[0m         \u001b[0mblocks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/chana/lib/python3.6/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mconcatenate_join_units\u001b[0;34m(join_units, concat_axis, copy)\u001b[0m\n\u001b[1;32m   5330\u001b[0m     to_concat = [ju.get_reindexed_values(empty_dtype=empty_dtype,\n\u001b[1;32m   5331\u001b[0m                                          upcasted_na=upcasted_na)\n\u001b[0;32m-> 5332\u001b[0;31m                  for ju in join_units]\n\u001b[0m\u001b[1;32m   5333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5334\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mto_concat\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/chana/lib/python3.6/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   5330\u001b[0m     to_concat = [ju.get_reindexed_values(empty_dtype=empty_dtype,\n\u001b[1;32m   5331\u001b[0m                                          upcasted_na=upcasted_na)\n\u001b[0;32m-> 5332\u001b[0;31m                  for ju in join_units]\n\u001b[0m\u001b[1;32m   5333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5334\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mto_concat\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/chana/lib/python3.6/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mget_reindexed_values\u001b[0;34m(self, empty_dtype, upcasted_na)\u001b[0m\n\u001b[1;32m   5630\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindexers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5631\u001b[0m                 values = algos.take_nd(values, indexer, axis=ax,\n\u001b[0;32m-> 5632\u001b[0;31m                                        fill_value=fill_value)\n\u001b[0m\u001b[1;32m   5633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5634\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/chana/lib/python3.6/site-packages/pandas/core/algorithms.py\u001b[0m in \u001b[0;36mtake_nd\u001b[0;34m(arr, indexer, axis, out, fill_value, mask_info, allow_fill)\u001b[0m\n\u001b[1;32m   1377\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'F'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1378\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1379\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1381\u001b[0m     func = _get_take_nd_function(arr.ndim, arr.dtype, out.dtype, axis=axis,\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Scale conts\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler().fit(pd.concat([train[num_cols], \n",
    "                                         val[num_cols], test[num_cols]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(INTERIM/'scaler.pkl', 'rb') as f:\n",
    "    scaler = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_features(df, scaler, num_cols):\n",
    "    scaled = scaler.transform(df[num_cols])\n",
    "    for i, col in enumerate(num_cols):\n",
    "        df[col] = scaled[:,i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.67 s, sys: 8.05 s, total: 14.7 s\n",
      "Wall time: 14.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "scale_features(train, scaler, num_cols)\n",
    "scale_features(val, scaler, num_cols)\n",
    "scale_features(test, scaler, num_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_szs = [int(train[col].max() + 1) for col in cat_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 5, 35, 3, 86, 3240, 9, 21, 3, 8, 15, 37, 101, 13, 329, 14, 6]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_szs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_szs = [(c, min(50, (c+1)//2)) for c in cat_szs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, 1),\n",
       " (5, 3),\n",
       " (35, 18),\n",
       " (3, 2),\n",
       " (86, 43),\n",
       " (3240, 50),\n",
       " (9, 5),\n",
       " (21, 11),\n",
       " (3, 2),\n",
       " (8, 4),\n",
       " (15, 8),\n",
       " (37, 19),\n",
       " (101, 50),\n",
       " (13, 7),\n",
       " (329, 50),\n",
       " (14, 7),\n",
       " (6, 3)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_szs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, emb_szs, n_cont, emb_drop, out_sz, szs, drops,\n",
    "                 use_bn=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embs = nn.ModuleList([\n",
    "            nn.Embedding(c, s) for c,s in emb_szs\n",
    "        ])\n",
    "        for emb in self.embs:\n",
    "            self.emb_init(emb)\n",
    "            \n",
    "        n_emb = sum(e.embedding_dim for e in self.embs)\n",
    "        self.n_emb, self.n_cont = n_emb, n_cont\n",
    "        szs = [n_emb + n_cont] + szs\n",
    "        \n",
    "        self.lins = nn.ModuleList([\n",
    "            nn.Linear(szs[i], szs[i+1]) for i in range(len(szs)-1)\n",
    "        ])\n",
    "        for o in self.lins: \n",
    "            nn.init.kaiming_normal_(o.weight.data)\n",
    "        \n",
    "        self.bns = nn.ModuleList([\n",
    "            nn.BatchNorm1d(sz) for sz in szs[1:]\n",
    "        ])        \n",
    "            \n",
    "        self.outp = nn.Linear(szs[-1], out_sz)\n",
    "        nn.init.kaiming_normal_(self.outp.weight.data)\n",
    "        \n",
    "        self.emb_drop = nn.Dropout(emb_drop)\n",
    "        self.drops = nn.ModuleList([\n",
    "            nn.Dropout(drop) for drop in drops\n",
    "        ])\n",
    "        self.bn = nn.BatchNorm1d(n_cont)\n",
    "        \n",
    "        self.use_bn = use_bn\n",
    "    \n",
    "    def forward(self, x_cat, x_cont):\n",
    "        if self.n_emb != 0:\n",
    "            x = [emb(x_cat[:,i]) for i,emb in enumerate(self.embs)]\n",
    "            x = torch.cat(x, 1)\n",
    "            x = self.emb_drop(x)\n",
    "        if self.n_cont != 0:\n",
    "            x2 = self.bn(x_cont)\n",
    "            x = torch.cat([x, x2], 1) if self.n_emb != 0 else x2\n",
    "        for lin, drop, bn in zip(self.lins, self.drops, self.bns):\n",
    "            x = F.relu(lin(x))\n",
    "            if self.use_bn:\n",
    "                x = bn(x)\n",
    "            x = drop(x)\n",
    "        return self.outp(x) # coupled with BCEWithLogitsLoss\n",
    "    \n",
    "    def emb_init(self, x):\n",
    "        # higher init range for low-dimensional embeddings\n",
    "        x = x.weight.data\n",
    "        sc = 2 / (x.size(1) + 1)\n",
    "        x.uniform_(-sc, sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabularDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, cat_cols, num_cols, target_col=None):\n",
    "        self.cats = df[cat_cols].values.astype(np.int64)\n",
    "        self.conts = df[num_cols].values.astype(np.float32)\n",
    "        self.target = df[target_col].values.astype(np.float32) if target_col \\\n",
    "                            else np.zeros((len(df),1)).astype(np.float32)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.target)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return [self.cats[idx], self.conts[idx], self.target[idx]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.76 s, sys: 2.52 s, total: 4.28 s\n",
      "Wall time: 4.28 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_ds = DataLoader(TabularDataset(train, cat_cols, num_cols, target_col), batch_size=128, shuffle=True)\n",
    "val_ds = DataLoader(TabularDataset(val, cat_cols, num_cols, target_col), batch_size=128)\n",
    "test_ds = DataLoader(TabularDataset(test, cat_cols, num_cols), batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_CUDA = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# emb_szs = [(2, 1),\n",
    "#  (5, 3),\n",
    "#  (36, 18),\n",
    "#  (3, 2),\n",
    "#  (86, 43),\n",
    "#  (3240, 50),\n",
    "#  (9, 5),\n",
    "#  (21, 11),\n",
    "#  (3, 2),\n",
    "#  (8, 4),\n",
    "#  (15, 8),\n",
    "#  (37, 19),\n",
    "#  (101, 50),\n",
    "#  (14, 7),\n",
    "#  (330, 50),\n",
    "#  (15, 8),\n",
    "#  (6, 3)]\n",
    "# num_cols = ['a'] * 51"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17, 51)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cat_cols), len(num_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNet(emb_szs, n_cont=len(num_cols), emb_drop=0.04, \n",
    "                  out_sz=1, szs=[1000, 500], drops=[0.001, 0.01],\n",
    "                  use_bn=True)\n",
    "if USE_CUDA:\n",
    "    model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(cats, conts, target, model, optimizer, criterion, train=True):\n",
    "    if train:\n",
    "        optimizer.zero_grad()\n",
    "    pred = model(cats, conts)\n",
    "    loss = criterion(pred.view(-1), target)\n",
    "    if train:\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, optimizer, criterion, train_loader, val_loader, \n",
    "                n_epochs, print_every=800, val_every=4000, USE_CUDA=False):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        train_loss, val_loss = 0, 0\n",
    "        for batch_idx, (cats, conts, target) in enumerate(train_loader):\n",
    "            cats, conts, target = Variable(cats), Variable(conts), Variable(target)\n",
    "            if USE_CUDA:\n",
    "                cats, conts, target = cats.cuda(), conts.cuda(), target.cuda()\n",
    "            train_loss += train_step(cats, conts, target, model, optimizer, \n",
    "                                     criterion, train=True)\n",
    "            if batch_idx % print_every == 0:\n",
    "                train_loss /= print_every\n",
    "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                        epoch, batch_idx * len(cats), len(train_loader.dataset),\n",
    "                        100. * batch_idx / len(train_loader), train_loss))\n",
    "                train_losses.append(train_loss)\n",
    "                train_loss = 0\n",
    "            if batch_idx % val_every == 0:\n",
    "                targets, preds = get_predictions(model, val_ds, USE_CUDA=False)\n",
    "                val_loss = nn.BCELoss()(targets, preds)\n",
    "                val_losses.append(val_loss)\n",
    "                print('Validation Loss: {val_loss:.6f}')\n",
    "                \n",
    "        print()\n",
    "                \n",
    "    return model, train_losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(INTERIM/'model_weights.pkl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/8102750 (0%)]\tLoss: 0.000944\n",
      "Train Epoch: 0 [102400/8102750 (1%)]\tLoss: 0.165147\n",
      "Train Epoch: 0 [204800/8102750 (3%)]\tLoss: 0.134937\n",
      "Train Epoch: 0 [307200/8102750 (4%)]\tLoss: 0.133561\n",
      "Train Epoch: 0 [409600/8102750 (5%)]\tLoss: 0.133027\n",
      "Train Epoch: 0 [512000/8102750 (6%)]\tLoss: 0.129652\n",
      "Train Epoch: 0 [614400/8102750 (8%)]\tLoss: 0.131317\n",
      "Train Epoch: 0 [716800/8102750 (9%)]\tLoss: 0.130191\n",
      "Train Epoch: 0 [819200/8102750 (10%)]\tLoss: 0.131893\n",
      "Train Epoch: 0 [921600/8102750 (11%)]\tLoss: 0.128500\n",
      "Train Epoch: 0 [1024000/8102750 (13%)]\tLoss: 0.129030\n",
      "Train Epoch: 0 [1126400/8102750 (14%)]\tLoss: 0.131570\n",
      "Train Epoch: 0 [1228800/8102750 (15%)]\tLoss: 0.131791\n",
      "Train Epoch: 0 [1331200/8102750 (16%)]\tLoss: 0.127530\n",
      "Train Epoch: 0 [1433600/8102750 (18%)]\tLoss: 0.128304\n",
      "Train Epoch: 0 [1536000/8102750 (19%)]\tLoss: 0.129086\n",
      "Train Epoch: 0 [1638400/8102750 (20%)]\tLoss: 0.126029\n",
      "Train Epoch: 0 [1740800/8102750 (21%)]\tLoss: 0.127278\n",
      "Train Epoch: 0 [1843200/8102750 (23%)]\tLoss: 0.126728\n",
      "Train Epoch: 0 [1945600/8102750 (24%)]\tLoss: 0.127412\n",
      "Train Epoch: 0 [2048000/8102750 (25%)]\tLoss: 0.126461\n",
      "Train Epoch: 0 [2150400/8102750 (27%)]\tLoss: 0.123378\n",
      "Train Epoch: 0 [2252800/8102750 (28%)]\tLoss: 0.126387\n",
      "Train Epoch: 0 [2355200/8102750 (29%)]\tLoss: 0.129594\n",
      "Train Epoch: 0 [2457600/8102750 (30%)]\tLoss: 0.124305\n",
      "Train Epoch: 0 [2560000/8102750 (32%)]\tLoss: 0.124500\n",
      "Train Epoch: 0 [2662400/8102750 (33%)]\tLoss: 0.125717\n",
      "Train Epoch: 0 [2764800/8102750 (34%)]\tLoss: 0.126847\n",
      "Train Epoch: 0 [2867200/8102750 (35%)]\tLoss: 0.123908\n",
      "Train Epoch: 0 [2969600/8102750 (37%)]\tLoss: 0.133540\n",
      "Train Epoch: 0 [3072000/8102750 (38%)]\tLoss: 0.125338\n",
      "Train Epoch: 0 [3174400/8102750 (39%)]\tLoss: 0.123670\n",
      "Train Epoch: 0 [3276800/8102750 (40%)]\tLoss: 0.127488\n",
      "Train Epoch: 0 [3379200/8102750 (42%)]\tLoss: 0.125318\n",
      "Train Epoch: 0 [3481600/8102750 (43%)]\tLoss: 0.124623\n",
      "Train Epoch: 0 [3584000/8102750 (44%)]\tLoss: 0.126328\n",
      "Train Epoch: 0 [3686400/8102750 (45%)]\tLoss: 0.123705\n",
      "Train Epoch: 0 [3788800/8102750 (47%)]\tLoss: 0.124884\n",
      "Train Epoch: 0 [3891200/8102750 (48%)]\tLoss: 0.127141\n",
      "Train Epoch: 0 [3993600/8102750 (49%)]\tLoss: 0.127080\n",
      "Train Epoch: 0 [4096000/8102750 (51%)]\tLoss: 0.124165\n",
      "Train Epoch: 0 [4198400/8102750 (52%)]\tLoss: 0.123330\n",
      "Train Epoch: 0 [4300800/8102750 (53%)]\tLoss: 0.125164\n",
      "Train Epoch: 0 [4403200/8102750 (54%)]\tLoss: 0.126712\n",
      "Train Epoch: 0 [4505600/8102750 (56%)]\tLoss: 0.127198\n",
      "Train Epoch: 0 [4608000/8102750 (57%)]\tLoss: 0.124683\n",
      "Train Epoch: 0 [4710400/8102750 (58%)]\tLoss: 0.124813\n",
      "Train Epoch: 0 [4812800/8102750 (59%)]\tLoss: 0.125644\n",
      "Train Epoch: 0 [4915200/8102750 (61%)]\tLoss: 0.125132\n",
      "Train Epoch: 0 [5017600/8102750 (62%)]\tLoss: 0.124376\n",
      "Train Epoch: 0 [5120000/8102750 (63%)]\tLoss: 0.124224\n",
      "Train Epoch: 0 [5222400/8102750 (64%)]\tLoss: 0.125332\n",
      "Train Epoch: 0 [5324800/8102750 (66%)]\tLoss: 0.123758\n",
      "Train Epoch: 0 [5427200/8102750 (67%)]\tLoss: 0.122814\n",
      "Train Epoch: 0 [5529600/8102750 (68%)]\tLoss: 0.126076\n",
      "Train Epoch: 0 [5632000/8102750 (70%)]\tLoss: 0.122367\n",
      "Train Epoch: 0 [5734400/8102750 (71%)]\tLoss: 0.124652\n",
      "Train Epoch: 0 [5836800/8102750 (72%)]\tLoss: 0.125488\n",
      "Train Epoch: 0 [5939200/8102750 (73%)]\tLoss: 0.122853\n",
      "Train Epoch: 0 [6041600/8102750 (75%)]\tLoss: 0.122598\n",
      "Train Epoch: 0 [6144000/8102750 (76%)]\tLoss: 0.127832\n",
      "Train Epoch: 0 [6246400/8102750 (77%)]\tLoss: 0.122597\n",
      "Train Epoch: 0 [6348800/8102750 (78%)]\tLoss: 0.124114\n",
      "Train Epoch: 0 [6451200/8102750 (80%)]\tLoss: 0.124589\n",
      "Train Epoch: 0 [6553600/8102750 (81%)]\tLoss: 0.123294\n",
      "Train Epoch: 0 [6656000/8102750 (82%)]\tLoss: 0.123602\n",
      "Train Epoch: 0 [6758400/8102750 (83%)]\tLoss: 0.124233\n",
      "Train Epoch: 0 [6860800/8102750 (85%)]\tLoss: 0.126982\n",
      "Train Epoch: 0 [6963200/8102750 (86%)]\tLoss: 0.122694\n",
      "Train Epoch: 0 [7065600/8102750 (87%)]\tLoss: 0.121507\n",
      "Train Epoch: 0 [7168000/8102750 (88%)]\tLoss: 0.124758\n",
      "Train Epoch: 0 [7270400/8102750 (90%)]\tLoss: 0.128017\n",
      "Train Epoch: 0 [7372800/8102750 (91%)]\tLoss: 0.124046\n",
      "Train Epoch: 0 [7475200/8102750 (92%)]\tLoss: 0.124227\n",
      "Train Epoch: 0 [7577600/8102750 (94%)]\tLoss: 0.129337\n",
      "Train Epoch: 0 [7680000/8102750 (95%)]\tLoss: 0.125863\n",
      "Train Epoch: 0 [7782400/8102750 (96%)]\tLoss: 0.123833\n",
      "Train Epoch: 0 [7884800/8102750 (97%)]\tLoss: 0.120955\n",
      "Train Epoch: 0 [7987200/8102750 (99%)]\tLoss: 0.123644\n",
      "Train Epoch: 0 [8089600/8102750 (100%)]\tLoss: 0.122602\n",
      "Train Epoch: 1 [0/8102750 (0%)]\tLoss: 0.000172\n",
      "Train Epoch: 1 [102400/8102750 (1%)]\tLoss: 0.123000\n",
      "Train Epoch: 1 [204800/8102750 (3%)]\tLoss: 0.124533\n",
      "Train Epoch: 1 [307200/8102750 (4%)]\tLoss: 0.122141\n",
      "Train Epoch: 1 [409600/8102750 (5%)]\tLoss: 0.122686\n",
      "Train Epoch: 1 [512000/8102750 (6%)]\tLoss: 0.124055\n",
      "Train Epoch: 1 [614400/8102750 (8%)]\tLoss: 0.121918\n",
      "Train Epoch: 1 [716800/8102750 (9%)]\tLoss: 0.123679\n",
      "Train Epoch: 1 [819200/8102750 (10%)]\tLoss: 0.124163\n",
      "Train Epoch: 1 [921600/8102750 (11%)]\tLoss: 0.123452\n",
      "Train Epoch: 1 [1024000/8102750 (13%)]\tLoss: 0.122774\n",
      "Train Epoch: 1 [1126400/8102750 (14%)]\tLoss: 0.125652\n",
      "Train Epoch: 1 [1228800/8102750 (15%)]\tLoss: 0.123188\n",
      "Train Epoch: 1 [1331200/8102750 (16%)]\tLoss: 0.121710\n",
      "Train Epoch: 1 [1433600/8102750 (18%)]\tLoss: 0.123365\n",
      "Train Epoch: 1 [1536000/8102750 (19%)]\tLoss: 0.120922\n",
      "Train Epoch: 1 [1638400/8102750 (20%)]\tLoss: 0.121646\n",
      "Train Epoch: 1 [1740800/8102750 (21%)]\tLoss: 0.123674\n",
      "Train Epoch: 1 [1843200/8102750 (23%)]\tLoss: 0.124402\n",
      "Train Epoch: 1 [1945600/8102750 (24%)]\tLoss: 0.121701\n",
      "Train Epoch: 1 [2048000/8102750 (25%)]\tLoss: 0.124540\n",
      "Train Epoch: 1 [2150400/8102750 (27%)]\tLoss: 0.121762\n",
      "Train Epoch: 1 [2252800/8102750 (28%)]\tLoss: 0.123876\n",
      "Train Epoch: 1 [2355200/8102750 (29%)]\tLoss: 0.123552\n",
      "Train Epoch: 1 [2457600/8102750 (30%)]\tLoss: 0.125330\n",
      "Train Epoch: 1 [2560000/8102750 (32%)]\tLoss: 0.125182\n",
      "Train Epoch: 1 [2662400/8102750 (33%)]\tLoss: 0.125743\n",
      "Train Epoch: 1 [2764800/8102750 (34%)]\tLoss: 0.120803\n",
      "Train Epoch: 1 [2867200/8102750 (35%)]\tLoss: 0.124461\n",
      "Train Epoch: 1 [2969600/8102750 (37%)]\tLoss: 0.126081\n",
      "Train Epoch: 1 [3072000/8102750 (38%)]\tLoss: 0.121616\n",
      "Train Epoch: 1 [3174400/8102750 (39%)]\tLoss: 0.124146\n",
      "Train Epoch: 1 [3276800/8102750 (40%)]\tLoss: 0.124206\n",
      "Train Epoch: 1 [3379200/8102750 (42%)]\tLoss: 0.120713\n",
      "Train Epoch: 1 [3481600/8102750 (43%)]\tLoss: 0.121192\n",
      "Train Epoch: 1 [3584000/8102750 (44%)]\tLoss: 0.124167\n",
      "Train Epoch: 1 [3686400/8102750 (45%)]\tLoss: 0.119535\n",
      "Train Epoch: 1 [3788800/8102750 (47%)]\tLoss: 0.125765\n",
      "Train Epoch: 1 [3891200/8102750 (48%)]\tLoss: 0.124386\n",
      "Train Epoch: 1 [3993600/8102750 (49%)]\tLoss: 0.121931\n",
      "Train Epoch: 1 [4096000/8102750 (51%)]\tLoss: 0.121697\n",
      "Train Epoch: 1 [4198400/8102750 (52%)]\tLoss: 0.123845\n",
      "Train Epoch: 1 [4300800/8102750 (53%)]\tLoss: 0.122115\n",
      "Train Epoch: 1 [4403200/8102750 (54%)]\tLoss: 0.122251\n",
      "Train Epoch: 1 [4505600/8102750 (56%)]\tLoss: 0.122921\n",
      "Train Epoch: 1 [4608000/8102750 (57%)]\tLoss: 0.120736\n",
      "Train Epoch: 1 [4710400/8102750 (58%)]\tLoss: 0.122693\n",
      "Train Epoch: 1 [4812800/8102750 (59%)]\tLoss: 0.122512\n",
      "Train Epoch: 1 [4915200/8102750 (61%)]\tLoss: 0.122063\n",
      "Train Epoch: 1 [5017600/8102750 (62%)]\tLoss: 0.118734\n",
      "Train Epoch: 1 [5120000/8102750 (63%)]\tLoss: 0.124635\n",
      "Train Epoch: 1 [5222400/8102750 (64%)]\tLoss: 0.122025\n",
      "Train Epoch: 1 [5324800/8102750 (66%)]\tLoss: 0.120591\n",
      "Train Epoch: 1 [5427200/8102750 (67%)]\tLoss: 0.123363\n",
      "Train Epoch: 1 [5529600/8102750 (68%)]\tLoss: 0.122057\n",
      "Train Epoch: 1 [5632000/8102750 (70%)]\tLoss: 0.122105\n",
      "Train Epoch: 1 [5734400/8102750 (71%)]\tLoss: 0.124670\n",
      "Train Epoch: 1 [5836800/8102750 (72%)]\tLoss: 0.121681\n",
      "Train Epoch: 1 [5939200/8102750 (73%)]\tLoss: 0.123683\n",
      "Train Epoch: 1 [6041600/8102750 (75%)]\tLoss: 0.121795\n",
      "Train Epoch: 1 [6144000/8102750 (76%)]\tLoss: 0.123470\n",
      "Train Epoch: 1 [6246400/8102750 (77%)]\tLoss: 0.123826\n",
      "Train Epoch: 1 [6348800/8102750 (78%)]\tLoss: 0.120854\n",
      "Train Epoch: 1 [6451200/8102750 (80%)]\tLoss: 0.123153\n",
      "Train Epoch: 1 [6553600/8102750 (81%)]\tLoss: 0.123416\n",
      "Train Epoch: 1 [6656000/8102750 (82%)]\tLoss: 0.124037\n",
      "Train Epoch: 1 [6758400/8102750 (83%)]\tLoss: 0.120609\n",
      "Train Epoch: 1 [6860800/8102750 (85%)]\tLoss: 0.120937\n",
      "Train Epoch: 1 [6963200/8102750 (86%)]\tLoss: 0.120099\n",
      "Train Epoch: 1 [7065600/8102750 (87%)]\tLoss: 0.123360\n",
      "Train Epoch: 1 [7168000/8102750 (88%)]\tLoss: 0.123525\n",
      "Train Epoch: 1 [7270400/8102750 (90%)]\tLoss: 0.121624\n",
      "Train Epoch: 1 [7372800/8102750 (91%)]\tLoss: 0.125254\n",
      "Train Epoch: 1 [7475200/8102750 (92%)]\tLoss: 0.124503\n",
      "Train Epoch: 1 [7577600/8102750 (94%)]\tLoss: 0.121766\n",
      "Train Epoch: 1 [7680000/8102750 (95%)]\tLoss: 0.123020\n",
      "Train Epoch: 1 [7782400/8102750 (96%)]\tLoss: 0.125361\n",
      "Train Epoch: 1 [7884800/8102750 (97%)]\tLoss: 0.121179\n",
      "Train Epoch: 1 [7987200/8102750 (99%)]\tLoss: 0.123602\n",
      "Train Epoch: 1 [8089600/8102750 (100%)]\tLoss: 0.121995\n",
      "CPU times: user 13min 45s, sys: 12.5 s, total: 13min 57s\n",
      "Wall time: 13min 55s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = train_model(model, optimizer, criterion, train_ds, val_ds, \n",
    "                    n_epochs=2, USE_CUDA=USE_CUDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), INTERIM/'model_weights.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(INTERIM/'train_ds.pkl', 'wb') as f:\n",
    "    pickle.dump(train_ds, f, pickle.HIGHEST_PROTOCOL)\n",
    "with open(INTERIM/'val_ds.pkl', 'wb') as f:\n",
    "    pickle.dump(val_ds, f, pickle.HIGHEST_PROTOCOL)\n",
    "with open(INTERIM/'test_ds.pkl', 'wb') as f:\n",
    "    pickle.dump(test_ds, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(model, data_loader, print_every=800, USE_CUDA=False):\n",
    "    all_targets = []\n",
    "    all_preds = []\n",
    "    model.eval()\n",
    "    for batch_idx, (cats, conts, target) in enumerate(data_loader):\n",
    "        cats, conts, target = Variable(cats), Variable(conts), Variable(target)\n",
    "        if USE_CUDA:\n",
    "            cats, conts, target = cats.cuda(), conts.cuda(), target.cuda()\n",
    "        preds = model(cats, conts)\n",
    "        all_targets.extend(target.cpu())\n",
    "        all_preds.extend(preds.cpu())\n",
    "        if batch_idx % print_every == 0:\n",
    "            print('[{}/{} ({:.0f}%)]'.format(\n",
    "                    batch_idx * len(cats), len(data_loader.dataset),\n",
    "                    100. * batch_idx / len(data_loader)))\n",
    "    return [x.item() for x in all_targets], [F.sigmoid(x).item() for x in all_preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/493590 (0%)]\n",
      "[102400/493590 (21%)]\n",
      "[204800/493590 (41%)]\n",
      "[307200/493590 (62%)]\n",
      "[409600/493590 (83%)]\n"
     ]
    }
   ],
   "source": [
    "targets, preds = get_predictions(model, val_ds, USE_CUDA=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8654110838684123"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(targets, preds) # validation auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/8102750 (0%)]\n",
      "[102400/8102750 (1%)]\n",
      "[204800/8102750 (3%)]\n",
      "[307200/8102750 (4%)]\n",
      "[409600/8102750 (5%)]\n",
      "[512000/8102750 (6%)]\n",
      "[614400/8102750 (8%)]\n",
      "[716800/8102750 (9%)]\n",
      "[819200/8102750 (10%)]\n",
      "[921600/8102750 (11%)]\n",
      "[1024000/8102750 (13%)]\n",
      "[1126400/8102750 (14%)]\n",
      "[1228800/8102750 (15%)]\n",
      "[1331200/8102750 (16%)]\n",
      "[1433600/8102750 (18%)]\n",
      "[1536000/8102750 (19%)]\n",
      "[1638400/8102750 (20%)]\n",
      "[1740800/8102750 (21%)]\n",
      "[1843200/8102750 (23%)]\n",
      "[1945600/8102750 (24%)]\n",
      "[2048000/8102750 (25%)]\n",
      "[2150400/8102750 (27%)]\n",
      "[2252800/8102750 (28%)]\n"
     ]
    }
   ],
   "source": [
    "targets, preds = get_predictions(model, train_ds, USE_CUDA=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(targets, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/484758 (0%)]\n",
      "[102400/484758 (21%)]\n",
      "[204800/484758 (42%)]\n",
      "[307200/484758 (63%)]\n",
      "[409600/484758 (84%)]\n"
     ]
    }
   ],
   "source": [
    "targets, preds = get_predictions(model, test_ds, USE_CUDA=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    484758.000000\n",
       "mean          0.020664\n",
       "std           0.039237\n",
       "min           0.000000\n",
       "25%           0.006615\n",
       "50%           0.013033\n",
       "75%           0.024082\n",
       "max           1.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(preds).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[target_col] = preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv(RAW/'sample_submission.csv', low_memory=False)\n",
    "submission = pd.merge(submission[['PredictionIdx']], test[['PredictionIdx', target_col]], \n",
    "                      how='left', on='PredictionIdx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    484758.000000\n",
       "mean          0.020664\n",
       "std           0.039237\n",
       "min           0.000000\n",
       "25%           0.006615\n",
       "50%           0.013033\n",
       "75%           0.024082\n",
       "max           1.000000\n",
       "Name: CustomerInterest, dtype: float64"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission[target_col].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PredictionIdx</th>\n",
       "      <th>CustomerInterest</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a1e0d80784</td>\n",
       "      <td>0.009377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>c2cc6cc2a8</td>\n",
       "      <td>0.032916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a8e94f6344</td>\n",
       "      <td>0.003766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>758bae1e35</td>\n",
       "      <td>0.000477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>02ab378ee8</td>\n",
       "      <td>0.002878</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  PredictionIdx  CustomerInterest\n",
       "0    a1e0d80784          0.009377\n",
       "1    c2cc6cc2a8          0.032916\n",
       "2    a8e94f6344          0.003766\n",
       "3    758bae1e35          0.000477\n",
       "4    02ab378ee8          0.002878"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fa5b65d56d8>"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa77608b860>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 248,
       "width": 394
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "submission[target_col].hist(bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(SUBMISSIONS/'RA03-26-neuralnet_2018_SVD_diffscounts.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.neuralnet import TabularDataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.39 s, sys: 6.38 s, total: 10.8 s\n",
      "Wall time: 10.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "all_train_ds = DataLoader(TabularDataset(pd.concat([train, val]), cat_cols, num_cols, target_col), \n",
    "                                                  batch_size=128, shuffle=True)\n",
    "test_ds = DataLoader(TabularDataset(test, cat_cols, num_cols), batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_CUDA = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.neuralnet import NeuralNet, train_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNet(emb_szs, n_cont=len(num_cols), emb_drop=0.04, \n",
    "                  szs=[1000, 500, 500], drops=[0.001, 0.01, 0.01],\n",
    "                  use_bn=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model, train_losses, val_losses, val_aucs = train_model(\n",
    "                                model, optimizer, criterion, \n",
    "                                all_train_ds, None, n_epochs=2, \n",
    "                                USE_CUDA=USE_CUDA, val_every=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/8596340 (0%)]\tLoss: 0.000955\n",
      "Train Epoch: 0 [102400/8596340 (1%)]\tLoss: 0.164358\n",
      "Train Epoch: 0 [204800/8596340 (2%)]\tLoss: 0.131999\n",
      "Train Epoch: 0 [307200/8596340 (4%)]\tLoss: 0.130099\n",
      "Train Epoch: 0 [409600/8596340 (5%)]\tLoss: 0.133853\n",
      "Train Epoch: 0 [512000/8596340 (6%)]\tLoss: 0.131810\n",
      "Train Epoch: 0 [614400/8596340 (7%)]\tLoss: 0.128595\n",
      "Train Epoch: 0 [716800/8596340 (8%)]\tLoss: 0.129635\n",
      "Train Epoch: 0 [819200/8596340 (10%)]\tLoss: 0.127177\n",
      "Train Epoch: 0 [921600/8596340 (11%)]\tLoss: 0.130097\n",
      "Train Epoch: 0 [1024000/8596340 (12%)]\tLoss: 0.129600\n",
      "Train Epoch: 0 [1126400/8596340 (13%)]\tLoss: 0.128196\n",
      "Train Epoch: 0 [1228800/8596340 (14%)]\tLoss: 0.126975\n",
      "Train Epoch: 0 [1331200/8596340 (15%)]\tLoss: 0.129437\n",
      "Train Epoch: 0 [1433600/8596340 (17%)]\tLoss: 0.129222\n",
      "Train Epoch: 0 [1536000/8596340 (18%)]\tLoss: 0.128111\n",
      "Train Epoch: 0 [1638400/8596340 (19%)]\tLoss: 0.125244\n",
      "Train Epoch: 0 [1740800/8596340 (20%)]\tLoss: 0.127451\n",
      "Train Epoch: 0 [1843200/8596340 (21%)]\tLoss: 0.129109\n",
      "Train Epoch: 0 [1945600/8596340 (23%)]\tLoss: 0.126156\n",
      "Train Epoch: 0 [2048000/8596340 (24%)]\tLoss: 0.127755\n",
      "Train Epoch: 0 [2150400/8596340 (25%)]\tLoss: 0.128806\n",
      "Train Epoch: 0 [2252800/8596340 (26%)]\tLoss: 0.126952\n",
      "Train Epoch: 0 [2355200/8596340 (27%)]\tLoss: 0.127638\n",
      "Train Epoch: 0 [2457600/8596340 (29%)]\tLoss: 0.126093\n",
      "Train Epoch: 0 [2560000/8596340 (30%)]\tLoss: 0.128088\n",
      "Train Epoch: 0 [2662400/8596340 (31%)]\tLoss: 0.125527\n",
      "Train Epoch: 0 [2764800/8596340 (32%)]\tLoss: 0.127999\n",
      "Train Epoch: 0 [2867200/8596340 (33%)]\tLoss: 0.127431\n",
      "Train Epoch: 0 [2969600/8596340 (35%)]\tLoss: 0.124243\n",
      "Train Epoch: 0 [3072000/8596340 (36%)]\tLoss: 0.123738\n",
      "Train Epoch: 0 [3174400/8596340 (37%)]\tLoss: 0.125233\n",
      "Train Epoch: 0 [3276800/8596340 (38%)]\tLoss: 0.125505\n",
      "Train Epoch: 0 [3379200/8596340 (39%)]\tLoss: 0.125848\n",
      "Train Epoch: 0 [3481600/8596340 (41%)]\tLoss: 0.124832\n",
      "Train Epoch: 0 [3584000/8596340 (42%)]\tLoss: 0.125320\n",
      "Train Epoch: 0 [3686400/8596340 (43%)]\tLoss: 0.126136\n",
      "Train Epoch: 0 [3788800/8596340 (44%)]\tLoss: 0.126710\n",
      "Train Epoch: 0 [3891200/8596340 (45%)]\tLoss: 0.126653\n",
      "Train Epoch: 0 [3993600/8596340 (46%)]\tLoss: 0.127389\n",
      "Train Epoch: 0 [4096000/8596340 (48%)]\tLoss: 0.126109\n",
      "Train Epoch: 0 [4198400/8596340 (49%)]\tLoss: 0.125163\n",
      "Train Epoch: 0 [4300800/8596340 (50%)]\tLoss: 0.124556\n",
      "Train Epoch: 0 [4403200/8596340 (51%)]\tLoss: 0.124455\n",
      "Train Epoch: 0 [4505600/8596340 (52%)]\tLoss: 0.123114\n",
      "Train Epoch: 0 [4608000/8596340 (54%)]\tLoss: 0.124572\n",
      "Train Epoch: 0 [4710400/8596340 (55%)]\tLoss: 0.124644\n",
      "Train Epoch: 0 [4812800/8596340 (56%)]\tLoss: 0.124323\n",
      "Train Epoch: 0 [4915200/8596340 (57%)]\tLoss: 0.123593\n",
      "Train Epoch: 0 [5017600/8596340 (58%)]\tLoss: 0.126035\n",
      "Train Epoch: 0 [5120000/8596340 (60%)]\tLoss: 0.126064\n",
      "Train Epoch: 0 [5222400/8596340 (61%)]\tLoss: 0.122199\n",
      "Train Epoch: 0 [5324800/8596340 (62%)]\tLoss: 0.123377\n",
      "Train Epoch: 0 [5427200/8596340 (63%)]\tLoss: 0.122384\n",
      "Train Epoch: 0 [5529600/8596340 (64%)]\tLoss: 0.123020\n",
      "Train Epoch: 0 [5632000/8596340 (66%)]\tLoss: 0.128632\n",
      "Train Epoch: 0 [5734400/8596340 (67%)]\tLoss: 0.123194\n",
      "Train Epoch: 0 [5836800/8596340 (68%)]\tLoss: 0.126839\n",
      "Train Epoch: 0 [5939200/8596340 (69%)]\tLoss: 0.125797\n",
      "Train Epoch: 0 [6041600/8596340 (70%)]\tLoss: 0.123361\n",
      "Train Epoch: 0 [6144000/8596340 (71%)]\tLoss: 0.123358\n",
      "Train Epoch: 0 [6246400/8596340 (73%)]\tLoss: 0.124825\n",
      "Train Epoch: 0 [6348800/8596340 (74%)]\tLoss: 0.123837\n",
      "Train Epoch: 0 [6451200/8596340 (75%)]\tLoss: 0.124710\n",
      "Train Epoch: 0 [6553600/8596340 (76%)]\tLoss: 0.122886\n",
      "Train Epoch: 0 [6656000/8596340 (77%)]\tLoss: 0.125735\n",
      "Train Epoch: 0 [6758400/8596340 (79%)]\tLoss: 0.124582\n",
      "Train Epoch: 0 [6860800/8596340 (80%)]\tLoss: 0.124487\n",
      "Train Epoch: 0 [6963200/8596340 (81%)]\tLoss: 0.122332\n",
      "Train Epoch: 0 [7065600/8596340 (82%)]\tLoss: 0.120281\n",
      "Train Epoch: 0 [7168000/8596340 (83%)]\tLoss: 0.121548\n",
      "Train Epoch: 0 [7270400/8596340 (85%)]\tLoss: 0.125057\n",
      "Train Epoch: 0 [7372800/8596340 (86%)]\tLoss: 0.121066\n",
      "Train Epoch: 0 [7475200/8596340 (87%)]\tLoss: 0.125642\n",
      "Train Epoch: 0 [7577600/8596340 (88%)]\tLoss: 0.126722\n",
      "Train Epoch: 0 [7680000/8596340 (89%)]\tLoss: 0.122219\n",
      "Train Epoch: 0 [7782400/8596340 (91%)]\tLoss: 0.120905\n",
      "Train Epoch: 0 [7884800/8596340 (92%)]\tLoss: 0.122704\n",
      "Train Epoch: 0 [7987200/8596340 (93%)]\tLoss: 0.123600\n",
      "Train Epoch: 0 [8089600/8596340 (94%)]\tLoss: 0.121275\n",
      "Train Epoch: 0 [8192000/8596340 (95%)]\tLoss: 0.123224\n",
      "Train Epoch: 0 [8294400/8596340 (96%)]\tLoss: 0.122396\n",
      "Train Epoch: 0 [8396800/8596340 (98%)]\tLoss: 0.125809\n",
      "Train Epoch: 0 [8499200/8596340 (99%)]\tLoss: 0.123362\n",
      "Train Epoch: 1 [0/8596340 (0%)]\tLoss: 0.000133\n",
      "Train Epoch: 1 [102400/8596340 (1%)]\tLoss: 0.124162\n",
      "Train Epoch: 1 [204800/8596340 (2%)]\tLoss: 0.121240\n",
      "Train Epoch: 1 [307200/8596340 (4%)]\tLoss: 0.126681\n",
      "Train Epoch: 1 [409600/8596340 (5%)]\tLoss: 0.123074\n",
      "Train Epoch: 1 [512000/8596340 (6%)]\tLoss: 0.124335\n",
      "Train Epoch: 1 [614400/8596340 (7%)]\tLoss: 0.124703\n",
      "Train Epoch: 1 [716800/8596340 (8%)]\tLoss: 0.121131\n",
      "Train Epoch: 1 [819200/8596340 (10%)]\tLoss: 0.121105\n",
      "Train Epoch: 1 [921600/8596340 (11%)]\tLoss: 0.121986\n",
      "Train Epoch: 1 [1024000/8596340 (12%)]\tLoss: 0.123940\n",
      "Train Epoch: 1 [1126400/8596340 (13%)]\tLoss: 0.123109\n",
      "Train Epoch: 1 [1228800/8596340 (14%)]\tLoss: 0.124810\n",
      "Train Epoch: 1 [1331200/8596340 (15%)]\tLoss: 0.122866\n",
      "Train Epoch: 1 [1433600/8596340 (17%)]\tLoss: 0.119840\n",
      "Train Epoch: 1 [1536000/8596340 (18%)]\tLoss: 0.119120\n",
      "Train Epoch: 1 [1638400/8596340 (19%)]\tLoss: 0.121303\n",
      "Train Epoch: 1 [1740800/8596340 (20%)]\tLoss: 0.122674\n",
      "Train Epoch: 1 [1843200/8596340 (21%)]\tLoss: 0.123303\n",
      "Train Epoch: 1 [1945600/8596340 (23%)]\tLoss: 0.122664\n",
      "Train Epoch: 1 [2048000/8596340 (24%)]\tLoss: 0.122914\n",
      "Train Epoch: 1 [2150400/8596340 (25%)]\tLoss: 0.126113\n",
      "Train Epoch: 1 [2252800/8596340 (26%)]\tLoss: 0.124629\n",
      "Train Epoch: 1 [2355200/8596340 (27%)]\tLoss: 0.121197\n",
      "Train Epoch: 1 [2457600/8596340 (29%)]\tLoss: 0.122554\n",
      "Train Epoch: 1 [2560000/8596340 (30%)]\tLoss: 0.122278\n",
      "Train Epoch: 1 [2662400/8596340 (31%)]\tLoss: 0.122276\n",
      "Train Epoch: 1 [2764800/8596340 (32%)]\tLoss: 0.119937\n",
      "Train Epoch: 1 [2867200/8596340 (33%)]\tLoss: 0.122794\n",
      "Train Epoch: 1 [2969600/8596340 (35%)]\tLoss: 0.120565\n",
      "Train Epoch: 1 [3072000/8596340 (36%)]\tLoss: 0.121297\n",
      "Train Epoch: 1 [3174400/8596340 (37%)]\tLoss: 0.119837\n",
      "Train Epoch: 1 [3276800/8596340 (38%)]\tLoss: 0.121954\n",
      "Train Epoch: 1 [3379200/8596340 (39%)]\tLoss: 0.122860\n",
      "Train Epoch: 1 [3481600/8596340 (41%)]\tLoss: 0.122397\n",
      "Train Epoch: 1 [3584000/8596340 (42%)]\tLoss: 0.124838\n",
      "Train Epoch: 1 [3686400/8596340 (43%)]\tLoss: 0.124271\n",
      "Train Epoch: 1 [3788800/8596340 (44%)]\tLoss: 0.121028\n",
      "Train Epoch: 1 [3891200/8596340 (45%)]\tLoss: 0.123456\n",
      "Train Epoch: 1 [3993600/8596340 (46%)]\tLoss: 0.120718\n",
      "Train Epoch: 1 [4096000/8596340 (48%)]\tLoss: 0.123359\n",
      "Train Epoch: 1 [4198400/8596340 (49%)]\tLoss: 0.119465\n",
      "Train Epoch: 1 [4300800/8596340 (50%)]\tLoss: 0.120490\n",
      "Train Epoch: 1 [4403200/8596340 (51%)]\tLoss: 0.125353\n",
      "Train Epoch: 1 [4505600/8596340 (52%)]\tLoss: 0.121435\n",
      "Train Epoch: 1 [4608000/8596340 (54%)]\tLoss: 0.123333\n",
      "Train Epoch: 1 [4710400/8596340 (55%)]\tLoss: 0.122868\n",
      "Train Epoch: 1 [4812800/8596340 (56%)]\tLoss: 0.121079\n",
      "Train Epoch: 1 [4915200/8596340 (57%)]\tLoss: 0.120728\n",
      "Train Epoch: 1 [5017600/8596340 (58%)]\tLoss: 0.121938\n",
      "Train Epoch: 1 [5120000/8596340 (60%)]\tLoss: 0.125589\n",
      "Train Epoch: 1 [5222400/8596340 (61%)]\tLoss: 0.123380\n",
      "Train Epoch: 1 [5324800/8596340 (62%)]\tLoss: 0.121974\n",
      "Train Epoch: 1 [5427200/8596340 (63%)]\tLoss: 0.121101\n",
      "Train Epoch: 1 [5529600/8596340 (64%)]\tLoss: 0.121830\n",
      "Train Epoch: 1 [5632000/8596340 (66%)]\tLoss: 0.121890\n",
      "Train Epoch: 1 [5734400/8596340 (67%)]\tLoss: 0.122605\n",
      "Train Epoch: 1 [5836800/8596340 (68%)]\tLoss: 0.123249\n",
      "Train Epoch: 1 [5939200/8596340 (69%)]\tLoss: 0.119178\n",
      "Train Epoch: 1 [6041600/8596340 (70%)]\tLoss: 0.126125\n",
      "Train Epoch: 1 [6144000/8596340 (71%)]\tLoss: 0.120465\n",
      "Train Epoch: 1 [6246400/8596340 (73%)]\tLoss: 0.120832\n",
      "Train Epoch: 1 [6348800/8596340 (74%)]\tLoss: 0.118440\n",
      "Train Epoch: 1 [6451200/8596340 (75%)]\tLoss: 0.120151\n",
      "Train Epoch: 1 [6553600/8596340 (76%)]\tLoss: 0.124261\n",
      "Train Epoch: 1 [6656000/8596340 (77%)]\tLoss: 0.122387\n",
      "Train Epoch: 1 [6758400/8596340 (79%)]\tLoss: 0.121576\n",
      "Train Epoch: 1 [6860800/8596340 (80%)]\tLoss: 0.122699\n",
      "Train Epoch: 1 [6963200/8596340 (81%)]\tLoss: 0.120266\n",
      "Train Epoch: 1 [7065600/8596340 (82%)]\tLoss: 0.119157\n",
      "Train Epoch: 1 [7168000/8596340 (83%)]\tLoss: 0.123671\n",
      "Train Epoch: 1 [7270400/8596340 (85%)]\tLoss: 0.121820\n",
      "Train Epoch: 1 [7372800/8596340 (86%)]\tLoss: 0.123200\n",
      "Train Epoch: 1 [7475200/8596340 (87%)]\tLoss: 0.119555\n",
      "Train Epoch: 1 [7577600/8596340 (88%)]\tLoss: 0.121197\n",
      "Train Epoch: 1 [7680000/8596340 (89%)]\tLoss: 0.124418\n",
      "Train Epoch: 1 [7782400/8596340 (91%)]\tLoss: 0.118763\n",
      "Train Epoch: 1 [7884800/8596340 (92%)]\tLoss: 0.120721\n",
      "Train Epoch: 1 [7987200/8596340 (93%)]\tLoss: 0.121550\n",
      "Train Epoch: 1 [8089600/8596340 (94%)]\tLoss: 0.118960\n",
      "Train Epoch: 1 [8192000/8596340 (95%)]\tLoss: 0.121892\n",
      "Train Epoch: 1 [8294400/8596340 (96%)]\tLoss: 0.121069\n",
      "Train Epoch: 1 [8396800/8596340 (98%)]\tLoss: 0.123365\n",
      "Train Epoch: 1 [8499200/8596340 (99%)]\tLoss: 0.121590\n",
      "CPU times: user 15min 17s, sys: 13.1 s, total: 15min 30s\n",
      "Wall time: 15min 28s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = train_model(model, optimizer, criterion, all_train_ds, val_ds, \n",
    "                    n_epochs=2, USE_CUDA=USE_CUDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), INTERIM/'model_weights_alltrain.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/484758 (0%)]\n",
      "[102400/484758 (21%)]\n",
      "[204800/484758 (42%)]\n",
      "[307200/484758 (63%)]\n",
      "[409600/484758 (84%)]\n"
     ]
    }
   ],
   "source": [
    "targets, preds = get_predictions(model, test_ds, USE_CUDA=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[target_col] = preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv(RAW/'sample_submission.csv', low_memory=False)\n",
    "submission = pd.merge(submission[['PredictionIdx']], test[['PredictionIdx', target_col]], \n",
    "                      how='left', on='PredictionIdx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    4.847580e+05\n",
       "mean     2.414028e-02\n",
       "std      5.604297e-02\n",
       "min      2.730480e-17\n",
       "25%      6.091856e-03\n",
       "50%      1.257137e-02\n",
       "75%      2.517368e-02\n",
       "max      1.000000e+00\n",
       "Name: CustomerInterest, dtype: float64"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission[target_col].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PredictionIdx</th>\n",
       "      <th>CustomerInterest</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a1e0d80784</td>\n",
       "      <td>0.005362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>c2cc6cc2a8</td>\n",
       "      <td>0.033340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a8e94f6344</td>\n",
       "      <td>0.003676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>758bae1e35</td>\n",
       "      <td>0.000211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>02ab378ee8</td>\n",
       "      <td>0.001596</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  PredictionIdx  CustomerInterest\n",
       "0    a1e0d80784          0.005362\n",
       "1    c2cc6cc2a8          0.033340\n",
       "2    a8e94f6344          0.003676\n",
       "3    758bae1e35          0.000211\n",
       "4    02ab378ee8          0.001596"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fa7799bf9e8>"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAxUAAAHwCAYAAADQC0ISAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAAWJQAAFiUBSVIk8AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAIABJREFUeJzt3X2UXVWZ7/tvhAYkBhA6HlG6RZA8topNt9BIOIQ3RbyKIIRD9KpgbDhqCC0EbQ4iqAdsWyOoROUcPAEVxw12HMKAbvAtYLAjNHDhcER8eE17sUEC2iQQAQO5f6y5dffOrqSq5q6q1N7fzxg1Fnvt+aw1K5NU9q/mWmtOWb9+PZIkSZI0Ws+b6A5IkiRJmtwMFZIkSZKqGCokSZIkVTFUSJIkSapiqJAkSZJUxVAhSZIkqYqhQpIkSVIVQ4UkSZKkKoYKSZIkSVUMFZIkSZKqGCokSZIkVTFUSJIkSapiqJAkSZJUZcuJ7oCGZ9WqNevH+5zTp09rnXu8T61x5Dj3P8d4MDjOg8FxHgwTOc7Tp0+bMpo6ZyokSZIkVTFUSJIkSapiqJAkSZJUxVAhSZIkqYqhQpIkSVIVQ4UkSZKkKoYKSZIkSVUMFZIkSZKqGCokSZIkVTFUSJIkSapiqJAkSZJUxVAhSZIkqYqhQpIkSVIVQ4UkSZKkKoYKSZIkSVUMFZIkSZKqGCokSZIkVTFUSJIkSapiqJAkSZJUZcuJ7oA2b0csuHJE7RefccgY9USSJEmbK2cqJEmSJFUxVEiSJEmqYqiQJEmSVMVQIUmSJKmKoUKSJElSFUOFJEmSpCqGCkmSJElVDBWSJEmSqhgqJEmSJFUxVEiSJEmqYqiQJEmSVMVQIUmSJKmKoUKSJElSFUOFJEmSpCqGCkmSJElVDBWSJEmSqhgqJEmSJFUxVEiSJEmqYqiQJEmSVMVQIUmSJKmKoUKSJElSFUOFJEmSpCqGCkmSJElVDBWSJEmSqhgqJEmSJFUxVEiSJEmqYqiQJEmSVMVQIUmSJKnKlr04SET8PbA3MAP4Y+C3wL8CVwCLMvOxLjUzgbOA1wPbAPcCi4ELM/PZIc7zVuB04C+ALYA7gS9n5tc20rfjgXnAq4BngduAhZl59RDttwDmA3OBPcr3ciNwbmauGKLm+cAZwBzgZcBq4HrgnMy8a6i+SZIkSf2gVzMVpwJTge8DXwC+CawDPg7cERF/0t44Io4ElgOzgO8AXwK2Ai4AlnQ7QUScDFwFvAa4DLgYeAlwaUQsHKJmIXApsHNpfxmwJ3BVOV5n+ynl/BeU/iwq/ZsFLC/97qzZunzfZ9OEiS8APwDeDtwSEft265skSZLUL3oyUwFsl5lPde6MiPOAM4H/Bnyw7NuO5gP+s8BBmXlL2f8xYBkwOyLmZOaStuPsCiwEfg3snZkry/5PAjcDCyLi25n5k7aamcAC4D5gn8z8Tdn/WeBWYGFEXN06VjEHmA2sAA5tfU8RcRHwY+DiiFiWmWvaak4D9geWAsdl5nOl5nKamZrFEbFna78kSZLUb3oyU9EtUBTfKts92vbNBqYDS1qBou0YZ5WXH+g4zlxga5pLqVa21fwG+FR5+f6Omtbr81qBotSspJkZ2Rp4b0dN67xntX9PmXkzcHnp9+zW/jKz0TrPR9qDQ2ZeCdxAc9nVgUiSJEl9qlczFUM5omzvaNt3SNle26X9cmAtMDMits7Mp4dRc01Hm+Gc5xrgY6XNOfD7y5hmlvPfMETNu0vNJWXf7sCfAndn5gND1BxQaq7r8v6wTZ8+raZ83EyWfmpDjl3/c4wHg+M8GBznwTCZxrmnoSIiTgdeAGxPc+P2f6YJFJ9ub1a2d3fWZ+a6iHgAeDWwG3DXMGoeiogngV0iYtvMXBsRU4GXAk9k5kNdunpP2c5o2/cKmpu/78/MdcOsGbJfG6mRJEmS+kqvZypOB/5T2+trgRMyc1Xbvu3L9vEhjtHav8MIa6aWdmvH8By9qBmVVavWbLpRj40mHU9EP1WnNc6OXf9yjAeD4zwYHOfBMJHjPNrZkZ6uU5GZL87MKcCLgaNpZhtui4i/HMFhppTt+jGuGY9zjLZfkiRJ0qQxJovfZeavMvM7wGHATsDX295u/fZ++w0KG9t1tBtJzephtu82wzCW/RpqJkOSJEma9MZ0Re3M/FfgZ8CrI+KPW7vLdoP7DCJiS+DlNGtc3N9+qI3U7Exz6dODmbm2nPdJ4JfAC8r7nVpPo2q/F+Jemsfc7lb6MZyaIfu1kRpJkiSpr4xpqCheUratVbKXle3hXdrOArYFVrQ9+WlTNW/uaDOqmnK+FeX8BwzzPPcBvwBmRMTLR9A3SZIkqW9Uh4qIeGVEvLjL/ueVxe9eRBMSWmtFLAUeBeZExN5t7bcBzi0vv9JxuEuAp4GTy0J4rZoX0iyuB3BRR03r9UdLu1bNrsC8crxLOmpa5z239KdVsw9wHLAK+HZrf2aubzvPZyLieW01R9KEk58BP0KSJEnqU714+tPhwGcjYjnNb+4fo3kC1IE0N2o/DJzYapyZqyPiRJpwcX1ELKFZKfttNI9oXUqz0BxtNQ9ExIeBLwK3lNWqn6FZiG4X4HPtq2mXmhURcT7Nitd3RMRSYCuacLAjML9jNW2AJTQ3mM+mucH8Kpp7Qo6jedzsiZm5uqPmfOCtpeamiPghzdoVx9I8iWquq2lLkiSpn/Xi8qcfAP+T5sP30cCHgWNogsIngFdn5s/aCzLzCprQsby0nQ/8jiYAzCkzAHTUXEgTPO4E3gOcRBNYTsjM07t1LDMXACeUdieVujuBIzJzUZf264F3lH6sK/06uvRzVlklu7PmaeANwCdpHh17KvBG4Apgn8y8qVvfJEmSpH4xZf16n3Y6GaxatWbcB2r69GkcsWCDHLVRi8/oXNhcmzufed7/HOPB4DgPBsd5MEzwOhVTNt1qQ+Nxo7YkSZKkPmaokCRJklTFUCFJkiSpiqFCkiRJUhVDhSRJkqQqhgpJkiRJVQwVkiRJkqoYKiRJkiRVMVRIkiRJqmKokCRJklTFUCFJkiSpiqFCkiRJUhVDhSRJkqQqhgpJkiRJVQwVkiRJkqoYKiRJkiRVMVRIkiRJqmKokCRJklTFUCFJkiSpiqFCkiRJUhVDhSRJkqQqhgpJkiRJVQwVkiRJkqoYKiRJkiRVMVRIkiRJqmKokCRJklTFUCFJkiSpiqFCkiRJUhVDhSRJkqQqhgpJkiRJVQwVkiRJkqoYKiRJkiRVMVRIkiRJqmKokCRJklTFUCFJkiSpiqFCkiRJUhVDhSRJkqQqhgpJkiRJVQwVkiRJkqoYKiRJkiRVMVRIkiRJqmKokCRJklTFUCFJkiSpiqFCkiRJUhVDhSRJkqQqhgpJkiRJVQwVkiRJkqoYKiRJkiRV2bL2ABGxE/B24C3AnsBLgWeA/wNcAlySmc+1td8VeGAjh7w8M+cMca7jgXnAq4BngduAhZl59RDttwDmA3OBPYDfAjcC52bmiiFqng+cAcwBXgasBq4HzsnMu4ao2RE4GzgK2Bl4DLgWODszH9zI9ypJkiRNer2YqTgWuBjYF7gJ+DzwbeA1wFeBb0XElC51/xv4RJevpd1OEhELgUtpPrRfDFxGE2KuioiTu7SfAiwBLgC2AhYB3wFmAcsj4sguNVsD36cJCKuBLwA/oAlNt0TEvl1qdgJ+AvwNcF85378A7wVujYjdun0/kiRJUr+onqkA7gbeBvxjx4zEmTQfro8BjqYJGu1uz8yPD+cEETETWEDzoX2fzPxN2f9Z4FZgYURcnZkr28rmALOBFcChmflUqbkI+DFwcUQsy8w1bTWnAfvTBJvjWt9PRFwOXAEsjog9279P4FPADOCCzDytrc+n0ISSLwOHD+f7lCRJkiaj6pmKzFyWmVd1fNAmMx8GLiovD6o8zfvL9rxWoCjnWAl8CdiaZmag3QfK9qxWoCg1NwOXA9NpQgfw+5mN1nk+0v79ZOaVwA00l10d2FYzFXg38CRwTsf5FwErgTc5WyFJkqR+NtY3av+ubNd1ee8lEfFfI+LMsn3tRo5zSNle2+W9azratC5jmgmspQkDm6wBdgf+FLg7M7vd89GtZj/g+cA/d8x4UELJ98rLg7scT5IkSeoLvbj8qauI2BJ4T3nZLQy8sXy111wPHJ+Zv2jbN5Xm5u8nMvOhLse5p2xntO17BbAFcH9mdgs03WqibO/u0r6XNaMyffq02kOMi8nST23Iset/jvFgcJwHg+M8GCbTOI/lTMWnaW7W/qfM/G7b/rXAfwdeB7ywfB0IXEdzmdQPS5Bo2b5sHx/iPK39O0ySGkmSJKmvjMlMRblJeQHwc5p7Dn4vMx+hebpSu+URcRjNDdT7An9Nc5PzSKwfQdvW06g2x5quVq1as+lGPTaadDwR/VSd1jg7dv3LMR4MjvNgcJwHw0SO82hnR3o+UxER82gCwc+AgzPz18OpK5cpfbW8nNX2Vuu3/dvTXbfZgk3VbDeBNZIkSVJf6WmoiIgP0Tz16Kc0geLhER5iVdn+/vKnzHwS+CXwgojYuUvNHmXbfl/DvTSL4+1W7u0YTk2W7VD3P/SqRpIkSeorPQsVEfG3NAu/3U4TKB4ZxWFeX7b3d+xfVrbd1nt4c0cbMvNpmvUptgUOGE4NzRoYvwBmRMTLh1lzI80q3ftHxH+YK4qI5wGHlZfXdTmeJEmS1Bd6Eioi4mM0N2bfSrPQ3KMbabtvRGzVZf8hwKnl5WUdb7fWu/hoRLywrWZXYB7wNHBJR81XyvbciNimrWYf4DiaWZHfL8iXmevbzvOZEgpaNUfShJOfAT9qq3kC+AbNzMrHO85/MrAr8N3M7AxJkiRJUt+ovlE7Io4HPklzudENwCkR0dlsZWZeWv7774FXl8fHPlj2vZY/rP/wscxc0V6cmSsi4nyaFa/viIilwFY04WBHYH7HatoAS2hW8p4N3BYRVwE7lZotgBMzc3VHzfnAW0vNTRHxQ5q1K46leWrV3M5F/oAzaZ5adVpE7EWzivifAUcCj9CEHkmSJKlv9eLpT61LhbYAPjREmx8Bl5b//gbwdmAfmkuK/gj4FfAtYFFmdlusjsxcEBF30MwAnAQ8B/y/wGcz8+ou7ddHxDtoLoOaC8wHngKWA+d2BpdS83REvAE4A3gnzczJauAK4JzM/FmXmsciYj+aFbWPopnReIxm5uTszHyws0aSJEnqJ1PWr69+2qnGwapVa8Z9oKZPn8YRC64cUc3iMw7ZdCNtVnw8Yf9zjAeD4zwYHOfBMMGPlJ2y6VYbGsvF7yRJkiQNAEOFJEmSpCqGCkmSJElVDBWSJEmSqhgqJEmSJFUxVEiSJEmqYqiQJEmSVMVQIUmSJKmKoUKSJElSFUOFJEmSpCqGCkmSJElVDBWSJEmSqhgqJEmSJFUxVEiSJEmqYqiQJEmSVMVQIUmSJKmKoUKSJElSFUOFJEmSpCqGCkmSJElVDBWSJEmSqhgqJEmSJFUxVEiSJEmqYqiQJEmSVMVQIUmSJKmKoUKSJElSFUOFJEmSpCqGCkmSJElVDBWSJEmSqhgqJEmSJFUxVEiSJEmqYqiQJEmSVMVQIUmSJKmKoUKSJElSFUOFJEmSpCqGCkmSJElVDBWSJEmSqhgqJEmSJFUxVEiSJEmqYqiQJEmSVMVQIUmSJKmKoUKSJElSFUOFJEmSpCqGCkmSJElVDBWSJEmSqhgqJEmSJFUxVEiSJEmqYqiQJEmSVMVQIUmSJKnKlrUHiIidgLcDbwH2BF4KPAP8H+AS4JLMfK5L3UzgLOD1wDbAvcBi4MLMfHaIc70VOB34C2AL4E7gy5n5tY3073hgHvAq4FngNmBhZl49RPstgPnAXGAP4LfAjcC5mbliiJrnA2cAc4CXAauB64FzMvOuofomSZIk9YNezFQcC1wM7AvcBHwe+DbwGuCrwLciYkp7QUQcCSwHZgHfAb4EbAVcACzpdpKIOBm4qhz3snLOlwCXRsTCIWoWApcCO5f2l9EEn6vK8TrbTynnv6D0Z1Hp3yxgeel3Z83WwPeBs2nCxBeAH9AErVsiYt9ufZMkSZL6RS9Cxd3A24BdMvP/zsz/lplzgVcC/x9wDHB0q3FEbEfzAf9Z4KDMfF9mfhjYC/gJMDsi5rSfICJ2BRYCvwb2zsx5mXkq8FrgPmBBROzXUTMTWFDef21mnpqZ84DXleMsLMdtNweYDawA9srMD2fm+4CDS38vjohpHTWnAfsDS4F9M/NvM/Od5TjbAosjwsvMJEmS1LeqP+xm5rLMvKrzEqfMfBi4qLw8qO2t2cB0YElm3tLW/imay6EAPtBxmrnA1sCizFzZVvMb4FPl5fs7alqvzyvtWjUraWZGtgbe21HTOu9ZpT+tmpuBy0u/Z7f2l5mN1nk+0v5nkJlXAjfQXHZ1IJIkSVKfGuvfoP+ubNe17TukbK/t0n45sBaYWS4rGk7NNR1tRlVTzjeznP+GYZ5nd+BPgbsz84ER9E2SJEnqG9U3ag8lIrYE3lNetn+wj7K9u7MmM9dFxAPAq4HdgLuGUfNQRDwJ7BIR22bm2oiYSnPD+BOZ+VCX7t1TtjPa9r2C5ubv+zNz3YYlXWuG7NdGakZl+vTOq642T5Oln9qQY9f/HOPB4DgPBsd5MEymcR7LmYpP09xU/U+Z+d22/duX7eND1LX27zCKmu07tmNxjtoaSZIkqa+MyUxFRJxCc5P0z4F3j7C89aSo9WNcMx7nGG2/NrBq1ZraQ4zYaNLxRPRTdVrj7Nj1L8d4MDjOg8FxHgwTOc6jnR3p+UxFRMyjeazqz4CDM/PXHU06ZxU6bdfRbiQ1q4fZvtsMw1j2a6iZDEmSJGnS62moiIgP0azt8FOaQPFwl2ZZthvcZ1Duw3g5zY3d9w+zZmdgKvBgZq4FyMwngV8CLyjvd9qjbNvvhbiX5rGxu5V+DKdmyH5tpEaSJEnqKz0LFRHxtzSLxt1OEygeGaLpsrI9vMt7s2jWdliRmU8Ps+bNHW1GVVPOt6Kc/4Bhnuc+4BfAjIh4+Qj6JkmSJPWNnoSKiPgYzY3ZtwKHZuajG2m+FHgUmBMRe7cdYxvg3PLyKx01lwBPAye3L1gXES8EziwvL+qoab3+aGnXqtkVmFeOd0lHTeu855b+tGr2AY4DVtGsFg5AZq5vO89n2he5K6tvH0BzGdiPkCRJkvpU9Y3aEXE88EmaS4duAE6JiM5mKzPzUoDMXB0RJ9KEi+sjYgnNCtdvo3lE61KaheZ+LzMfiIgPA18EbomIy4FnaBai2wX4XGb+pKNmRUScT7Pi9R0RsRTYiiYc7AjMb19Ir1hCs/r3bOC2iLgK2KnUbAGcmJmrO2rOB95aam6KiB/SrF1xLM2aF3M7FwaUJEmS+kkvZipal/1sAXwIOKfL1wntBZl5Bc0q08uBY4D5NAvlnQbMKTMAdNRcSBM87qRZ/+Ik4GHghMw8vVvHMnNBOffDpf17Sv0RmbmoS/v1wDtKP9aVfh1d+jmrrJLdWfM08AaaYLUDcCrwRuAKYJ/MvKlb3yRJkqR+MWX9+uqnnWocrFq1ZtwHavr0aRyxYIMctVGLz3Dx8MnGxxP2P8d4MDjOg8FxHgwT/EjZKZtutaGxXPxOkiRJ0gAwVEiSJEmqYqiQJEmSVMVQIUmSJKmKoUKSJElSFUOFJEmSpCqGCkmSJElVDBWSJEmSqhgqJEmSJFUxVEiSJEmqYqiQJEmSVMVQIUmSJKmKoUKSJElSFUOFJEmSpCqGCkmSJElVDBWSJEmSqhgqJEmSJFUxVEiSJEmqYqiQJEmSVMVQIUmSJKmKoUKSJElSFUOFJEmSpCqGCkmSJElVDBWSJEmSqhgqJEmSJFUxVEiSJEmqYqiQJEmSVMVQIUmSJKmKoUKSJElSFUOFJEmSpCqGCkmSJElVDBWSJEmSqhgqJEmSJFUxVEiSJEmqYqiQJEmSVMVQIUmSJKmKoUKSJElSFUOFJEmSpCqGCkmSJElVDBWSJEmSqhgqJEmSJFUxVEiSJEmqYqiQJEmSVMVQIUmSJKmKoUKSJElSFUOFJEmSpCqGCkmSJElVDBWSJEmSqhgqJEmSJFXZshcHiYjZwIHAXsCfA9OAb2bmu7q03RV4YCOHuzwz5wxxnuOBecCrgGeB24CFmXn1EO23AOYDc4E9gN8CNwLnZuaKIWqeD5wBzAFeBqwGrgfOycy7hqjZETgbOArYGXgMuBY4OzMf3Mj3KkmSJE16PQkVwFk0YeIJ4EHglcOo+d/AFV32/7Rb44hYCCwox78Y2Irmg/9VETE/Mxd1tJ8CLAFmAwksAnYEjgOWR8QxmXllR83WwPeB/YFbgC8AfwIcC7wlIg7JzJs6anYCVgAzgGXlnK8E3ltq9svM+4fx5yFJkiRNSr0KFafSfNi/l2bG4rph1NyemR8fzsEjYiZNoLgP2Cczf1P2fxa4FVgYEVdn5sq2sjk0gWIFcGhmPlVqLgJ+DFwcEcsyc01bzWk0gWIpcFxmPldqLqcJQIsjYs/W/uJTNIHigsw8ra3Pp9CEki8Dhw/n+5QkSZImo57cU5GZ12XmPZm5vhfH6+L9ZXteK1CU864EvgRsTTMz0O4DZXtWK1CUmpuBy4HpNKED+P3MRus8H2kPDmVG4waay64ObKuZCrwbeBI4p+P8i4CVwJsiYrfhf6uSJEnS5NKrmYrReElE/FdgJ5p7EH6SmXcM0faQsr22y3vXAB8rbc6B31/GNBNYSxMGutW8u9RcUvbtDvwpcHdmdrvn4xrggFLTmonZD3g+8L2OGQ8y87mI+B5wEnAwUHUJ1PTp02rKx81k6ac25Nj1P8d4MDjOg8FxHgyTaZwnMlS8sXz9XkRcDxyfmb9o2zcVeCnwRGY+1OU495TtjLZ9rwC2AO7PzHXDrImyvXuI/vaqRpIkSeorExEq1gL/neYehdZv718LfJzmN/o/jIi9MvPJ8t72Zfv4EMdr7d+hbd/mXDMqq1at2XSjHhtNOp6IfqpOa5wdu/7lGA8Gx3kwOM6DYSLHebSzI+MeKjLzEZrHr7ZbHhGH0dxAvS/w1zQ3OY/ESO7nmLIZ10iSJEmTymaz+F25TOmr5eWstrdav+3fnu66zRZsqma7CayRJEmS+spmEyqKVWU7tbWjXAb1S+AFEbFzl5o9yrb9voZ7aRbH2y0ius3GdKvJsh3q/ode1UiSJEl9ZXMLFa8v284nJS0r227rPby5ow2Z+TTN+hTb0jyxaZM1NGtg/AKYEREvH2bNjTSrdO8fEf/hArSIeB5wWHk5nHU7JEmSpElp3ENFROwbEVt12X8IzSJ6AJd1vH1R2X40Il7YVrMrMA94mj88GrblK2V7bkRs01azD82q2quAb7f2lzU2Wuf5TAkFrZojacLJz4AftdU8AXyDZmbl4x3nPxnYFfiuK2pLkiSpn/XkRu2IOAo4qrx8cdnuFxGXlv9+NDNPL//998Cry+NjHyz7Xssf1qL4WGauaD9+Zq6IiPNpVry+IyKWAlvRhIMdgfkdq2kDLAGOplng7raIuIpmTYzjaB43e2Jmru6oOR94a6m5KSJ+SLN2xbE0T62a27GaNsCZwEHAaRGxF/AvwJ8BRwKP0IQeSZIkqW/1aqZiL+D48vWmsm+3tn2z29p+A7gJ2Ac4Efggzb0H3wJmZea53U6QmQuAE4CHaRaUew9wJ3BEZi7q0n498A6aILIOmE8TMpaX81zZpeZp4A3AJ2keA3sqzVoaVwD7ZOZNXWoeo1kE74s062MsoHmC1SXA6zLzvm7fjyRJktQvpqxf79NOJ4NVq9aM+0BNnz6NIxZskL02avEZh2y6kTYrPvO8/znGg8FxHgyO82CY4HUqpmy61YY2txu1JUmSJE0yhgpJkiRJVQwVkiRJkqoYKiRJkiRVMVRIkiRJqmKokCRJklTFUCFJkiSpiqFCkiRJUhVDhSRJkqQqhgpJkiRJVQwVkiRJkqoYKiRJkiRVMVRIkiRJqmKokCRJklTFUCFJkiSpiqFCkiRJUhVDhSRJkqQqhgpJkiRJVQwVkiRJkqoYKiRJkiRVMVRIkiRJqmKokCRJklTFUCFJkiSpiqFCkiRJUhVDhSRJkqQqhgpJkiRJVQwVkiRJkqoYKiRJkiRVMVRIkiRJqmKokCRJklTFUCFJkiSpiqFCkiRJUhVDhSRJkqQqhgpJkiRJVQwVkiRJkqoYKiRJkiRVMVRIkiRJqmKokCRJklTFUCFJkiSpiqFCkiRJUhVDhSRJkqQqhgpJkiRJVQwVkiRJkqoYKiRJkiRVMVRIkiRJqmKokCRJklTFUCFJkiSpiqFCkiRJUpUte3GQiJgNHAjsBfw5MA34Zma+ayM1M4GzgNcD2wD3AouBCzPz2SFq3gqcDvwFsAVwJ/DlzPzaRs5zPDAPeBXwLHAbsDAzrx6i/RbAfGAusAfwW+BG4NzMXDFEzfOBM4A5wMuA1cD1wDmZeddQfZMkSZL6Qa9mKs4CTqYJFb/cVOOIOBJYDswCvgN8CdgKuABYMkTNycBVwGuAy4CLgZcAl0bEwiFqFgKXAjuX9pcBewJXleN1tp9Szn9B6c+i0r9ZwPLS786arYHvA2fThIkvAD8A3g7cEhH7burPQ5IkSZrMehUqTgVmANsBH9hYw4jYjuYD/rPAQZn5vsz8ME0g+QkwOyLmdNTsCiwEfg3snZnzMvNU4LXAfcCCiNivo2YmsKC8/9rMPDUz5wGvK8dZWI7bbg4wG1gB7JWZH87M9wEHl/5eHBHTOmpOA/YHlgL7ZubfZuY7y3G2BRZHhJeZSZIkqW/15MNuZl6Xmfdk5vphNJ8NTAeWZOYtbcd4imbGAzYMJnOBrYFFmbmyreY3wKfKy/d31LRen1fatWpW0syMbA28t6Omdd6zSn9aNTcDl5d+z27tLzMbrfN8JDOfa6u5EriB5rKrA5EkSZL61ET8Bv2Qsr22y3vLgbXAzHJZ0XBqruloM6qacr6Z5fw3DPM8uwN/CtydmQ+MoG+SJElS3+jJjdojFGV7d+cbmbkuIh4AXg3sBtw1jJqHIuJJYJeI2DYz10bEVOClwBOZ+VCXPtxTtjPa9r2C5ubv+zNz3TCcxpviAAAUPklEQVRrhuzXRmpGZfr0zquuNk+TpZ/akGPX/xzjweA4DwbHeTBMpnGeiJmK7cv28SHeb+3fYRQ123dsx+IctTWSJElSX5mImYpNmVK2w7k/o6ZmPM4x2n5tYNWqNbWHGLHRpOOJ6KfqtMbZsetfjvFgcJwHg+M8GCZynEc7OzIRMxWdswqdtutoN5Ka1cNs322GYSz7NdRMhiRJkjTpTUSoyLLd4D6DiNgSeDmwDrh/mDU7A1OBBzNzLUBmPkmzXsYLyvud9ijb9nsh7qV5bOxupR/DqRmyXxupkSRJkvrKRISKZWV7eJf3ZtGs7bAiM58eZs2bO9qMqqacb0U5/wHDPM99wC+AGRHx8hH0TZIkSeobExEqlgKPAnMiYu/WzojYBji3vPxKR80lwNPAye0L1kXEC4Ezy8uLOmparz9a2rVqdgXmleNd0lHTOu+5pT+tmn2A44BVwLdb+8u6HK3zfKZ9kbuy+vYBwM+AHyFJkiT1qSnr11ffQ0xEHAUcVV6+GHgTzeVLrfUeHs3M0zvaLwWeApbQrHD9NppHtC4F/kvnQnoRMR/4IvAYzUJ0z9AsRLcL8Ln247fVfI5mxesHy3G3ogkHOwHzM3NRR/spwLfKcX8OXFXaHgdsAxxTFrVrr9maZiZiJnAL8EOatSuOLX08JDNv2tif33CsWrWmfqBGaPr0aRyx4MpNN6yw+AyX8Jho3vTX/xzjweA4DwbHeTBM8I3aUzbdakO9mqnYCzi+fL2p7Nutbd/s9saZeQXNKtPLgWOA+cDvaALAnG4rc2fmhTTB407gPcBJwMPACd0CRalZAJxQ2p1U6u4EjugMFKX9euAdpR/rSr+OLv2c1RkoSs3TwBuAT9I8OvZU4I3AFcA+vQgUkiRJ0uasJzMVGnvOVGis+Fuv/ucYDwbHeTA4zoNhkGcqJEmSJA0oQ4UkSZKkKoYKSZIkSVUMFZIkSZKqGCokSZIkVTFUSJIkSapiqJAkSZJUxVAhSZIkqYqhQpIkSVIVQ4UkSZKkKoYKSZIkSVUMFZIkSZKqGCokSZIkVTFUSJIkSapiqJAkSZJUxVAhSZIkqYqhQpIkSVIVQ4UkSZKkKoYKSZIkSVUMFZIkSZKqGCokSZIkVTFUSJIkSapiqJAkSZJUxVAhSZIkqYqhQpIkSVIVQ4UkSZKkKoYKSZIkSVUMFZIkSZKqGCokSZIkVTFUSJIkSapiqJAkSZJUxVAhSZIkqYqhQpIkSVIVQ4UkSZKkKoYKSZIkSVUMFZIkSZKqGCokSZIkVTFUSJIkSapiqJAkSZJUxVAhSZIkqYqhQpIkSVIVQ4UkSZKkKoYKSZIkSVUMFZIkSZKqGCokSZIkVTFUSJIkSapiqJAkSZJUxVAhSZIkqcqWE3XiiFgJvGyIt3+VmS/uUjMTOAt4PbANcC+wGLgwM58d4jxvBU4H/gLYArgT+HJmfm0jfTsemAe8CngWuA1YmJlXD9F+C2A+MBfYA/gtcCNwbmauGOo8kiRJUj+YsFBRPA58vsv+Jzp3RMSRwLeBp4DLgV8DRwAXAPsDx3apORm4EHgMuAx4BpgNXBoRe2bm6V1qFgILgAeBi4GtgDnAVRExPzMXdbSfAiwpx01gEbAjcBywPCKOycwrN/knIUmSJE1SEx0q/j0zP76pRhGxHc0H/GeBgzLzlrL/Y8AyYHZEzMnMJW01uwILacLH3pm5suz/JHAzsCAivp2ZP2mrmUkTKO4D9snM35T9nwVuBRZGxNWtYxVzaALFCuDQzHyq1FwE/Bi4OCKWZeaaEf7ZSJIkSZPCZLmnYjYwHVjSChQA5QP8WeXlBzpq5gJbA4vaQ0AJCp8qL9/fUdN6fV4rUJSalcCXyvHe21HTOu9ZrUBRam6mmVGZXvovSZIk9aWJDhVbR8S7IuLMiPibiDi43J/Q6ZCyvbbLe8uBtcDMiNh6mDXXdLQZVU0538xy/htGcB5JkiSpb0z05U8vBr7Rse+BiHhvZv6obV+U7d2dB8jMdRHxAPBqYDfgrmHUPBQRTwK7RMS2mbk2IqYCLwWeyMyHuvT1nrKd0bbvFTQ3f9+fmeuGWTMq06dPqz3EZqlfv6/JyLHof47xYHCcB4PjPBgm0zhP5EzFJcChNMFiKrAn8D+AXYFrIuLP29puX7aPD3Gs1v4dRlGzfcd2LM6xwxDvS5IkSZPehM1UZOYnOnb9FHh/RDxBc7P0x4G3D/NwU8p2/Qi6MJqa8TrHBlatGv/7vMcjHU/E96X/qDXOjkX/cowHg+M8GBznwTCR4zzaz38TfU9FNxeV7ay2fZ2zCp2262g3kprVw2zfbVZiNP2SJEmS+srmGCoeKdupbfuybDe4NyEitgReDqwD7h9mzc7l+A9m5lqAzHwS+CXwgvJ+pz3Ktv0ejXtpHnO7W+nHcGokSZKkvrI5hor9yrY9ICwr28O7tJ8FbAusyMynh1nz5o42o6op51tRzn/ACM4jSZIk9Y0JCRUR8eqI2LHL/pfRrEgNzQrYLUuBR4E5EbF3W/ttgHPLy690HO4S4Gng5LIQXqvmhcCZ5eVFHTWt1x8t7Vo1uwLzyvEu6ahpnffc0p9WzT40q2qvolkJXJIkSepLE3Wj9rHAGRFxHfAAsAbYHXgLsA3wTzSrYQOQmasj4kSacHF9RCyhWSn7bTSPjl1Ks9AcbTUPRMSHgS8Ct0TE5cAzNAvR7QJ8rn017VKzIiLOB04D7oiIpcBWNOFgR2B+x2raAEuAo8txb4uIq4CdSs0WwImZuRpJkiSpT03U5U/XAd+huRfinTQf4g8EfgwcD7w1M59pL8jMK0qb5cAxwHzgd6V2TmZu8ISlzLyQJnjcCbwHOAl4GDghM0/v1rHMXACcUNqdVOruBI7IzEVd2q8H3lH6sa706+jSz1mZeeUw/0wkSZKkSWnK+vXVTzvVOFi1as24D9T06dM4YsHYZqLFZ7jY+ETz8YT9zzEeDI7zYHCcB8MEP1J2yqZbbWhzvFFbkiRJ0iRiqJAkSZJUxVAhSZIkqYqhQpIkSVIVQ4UkSZKkKoYKSZIkSVUMFZIkSZKqGCokSZIkVTFUSJIkSapiqJAkSZJUxVAhSZIkqYqhQpIkSVIVQ4UkSZKkKoYKSZIkSVUMFZIkSZKqGCokSZIkVdlyojugwTb308tG1H7xGYeMUU8kSZI0Ws5USJIkSapiqJAkSZJUxVAhSZIkqYqhQpIkSVIVQ4UkSZKkKoYKSZIkSVUMFZIkSZKqGCokSZIkVTFUSJIkSapiqJAkSZJUxVAhSZIkqYqhQpIkSVIVQ4UkSZKkKoYKSZIkSVUMFZIkSZKqGCokSZIkVTFUSJIkSapiqJAkSZJUxVAhSZIkqYqhQpIkSVIVQ4UkSZKkKoYKSZIkSVW2nOgOSCMx99PLRlyz+IxDxqAnkiRJanGmQpIkSVIVQ4UkSZKkKoYKSZIkSVUMFZIkSZKqGCokSZIkVTFUSJIkSapiqJAkSZJUxXUq1PdGuraF61pIkiSNjDMVkiRJkqo4U9EjEbEL8EngcGAn4CHgCuATmfmbieybJEmSNJYMFT0QEbsDK4AXAVcCPwf+Cvgb4PCI2D8zH5vALmoEvFxKkiRpZAwVvfFlmkBxSmZe2NoZEecDpwLnAe+foL5JkiRJY8pQUSkidgMOA1YCX+p4+xzgJODdEbEgM58c5+5pHIx0ZmOknAmRJEmbO0NFvdYnvu9l5nPtb2Tmmoj4Z5rQ8Xrgh+PdOU1+Yx1aAK763JFjfg5JkgbZSP89n2z/Nhsq6kXZ3j3E+/fQhIoZVISK6dOnjbZU2qQjFlw50V3QZmiy/YOmhv9eDAbHeTBMpnE2VNTbvmwfH+L91v4dKs8zpbJ+VPxQIUmSVK/fP1O5TsXYa4WB9RPaC0mSJGmMGCrqtWYith/i/e062kmSJEl9xVBRL8t2xhDv71G2Q91zIUmSJE1qhop615XtYRHxH/48I2IasD/wW+DG8e6YJEmSNB4MFZUy8z7ge8CuwLyOtz8BTAW+7hoVkiRJ6ldT1q/3/uFaEbE7sIJmVe0rgbuAfYGDaS57mpmZj01cDyVJkqSxY6jokYj4E+CTwOHATsBDwBXAJzLz1xPZN0mSJGksGSokSZIkVfGeCkmSJElVDBWSJEmSqhgqJEmSJFUxVEiSJEmqYqiQJEmSVMVQIUmSJKnKlhPdAY2fiNiFodfS+M0IjrMjcDZwFLAz8BhwLXB2Zj7Y635rZGrHOSKm0oztW4C/BP4EeA5I4P8BLszMZ8am9xquXv197jjmLOA6ml84nZeZZ/WouxqlXo5zROwJfJhmYdYXAY/TLNb6vzLz673st4avh/82/2ea8f1z4MXAI8BPgS9m5rW97reGLyJmAwcCe9GMzzTgm5n5rlEcq+c/+3vFdSoGRJdVv38O/BXNPy4J7D+cVb8jYqdynBnAMuBm4JXAkTQ/wPbLzPvH4nvQpvVinCPicOAa4Nc0HzDvBXYEjqD5h2oFcGhmPjVG34Y2oVd/nzuOOQ24A/hj4AUYKiZcL8c5Ik4AvgqsBa4GVgI7AK8B/i0z5/S4+xqGHv7b/AHgy8CTwHeAB4FdgKOBbYGzMvO8sfgetGkRcTtNmHiCZmxeyShCxVj87O8lZyoGx5dp/ic8JTMvbO2MiPOBU4HzgPcP4zifogkUF2TmaW3HOQX4QjnP4T3st0amF+P8MPAu4B/aZyTKh87rgZnAPOBzPe25RqJXf5/bfQHYHvi7Uq+J15NxjojX0wSKnwKHZ+bDHe//US87rRGpHuMyfn8HPAW8LjOz7b1PAbcBH42IhZn5dO+/BQ3DqTRh4l6aGYvrRnmcsfjZ3zPeUzEAImI34DCa30x9qePtc2h+s/HuctnLxo4zFXh3aX9Ox9uLyvHfVM6ncdarcc7M2zPzm52XOGXmGv4QJA7qRZ81cr0a545jHgm8FzgF+Lfe9FQ1ejzOnwG2AN7VGSgAMvN3db3VaPRwjHek+YXA3e2BAiAz7wLuBp5PMwOpCZCZ12XmPZk56suDxuJnf68ZKgbDIWX7vcx8rv2N8kHxn2mmR1+/iePsR/OD6Z9LXftxngO+V14eXN1jjUavxnljWh8+1lUcQ3V6Os4R8SLgYuCKzLyslx1VlZ6Mc7n++gDgFuDOiDg4Ik6PiAURcWhE+Dlg4vTq7/IjwCpgRkTs0f5GRMwA9gBun8jLYtQT4/FvfBV/mAyGKNu7h3j/nrKdMU7H0dgYj/GZW7be9Ddxej3O/5Pm34IJmzJXV70a533a2i8rX58FFgI/AG6PiFdU9FOj15MxLr/9nkfz9/jWiPhaRPxdRHwduBW4Ezi2B/3VxNrsP4MZKgbD9mX7+BDvt/bvME7H0dgY0/GJiJNp7pe5HVg8mmOoJ3o2zhExl+YhCx/MzF/1oG/qnV6N84vK9r8Af0Zz4+72wCuAbwB7Av8YEVuNvqsapZ79Xc7Mf6D5Tfa/A+8BzuAPlytfAvgAlclvs/8MZqgQwJSyrX0UWK+Oo7Ex6vGJiKOBz9PcxH2M12Bv1oY1zhGxK82Y/kNmfmusO6WeG+7f5y3atn+dmd/JzNWZeR9wPM1lUTOAY8amm6ow7J/ZEfEumpmnG2jC47Zl+0Oaex6XjFEftfmY8M9ghorB0Eqv2w/x/nYd7cb6OBobYzI+EXEUzT9IjwAH+cjgCdercV4M/Bb4YC86pZ7r1Ti3nlv/NPBP7W+Uy2auLC//aqQdVLWejHG5b2IxzWVO787Mn2fmbzPz5zSzFbcCx0bEQfVd1gTa7D+DGSoGQ+tpEENdZ9e6sWuo6/R6fRyNjZ6PT0QcC/wD8CvgwM4ni2hC9Gqc/5Lm0phVEbG+9UVzqQQ0j6BcHxFX1HVXo9Trn9trOm/uLFqh4/kj6Jt6o1djfBjwR8CPutzA+xywvLx83Wg6qc3GZv8ZzFAxGFrPQz6s80kfZe2B/Wl+Y3njJo5zY2m3f6lrP87zaH6wtZ9P46tX49yqeSfNCtr/RhMo7tlEicZHr8b568D/6vLV+gBye3n9/d50WyPUq3G+A3gU+OOI+E9d3n9N2a4cfVc1Sr0a463LdvoQ77f2PzPE+5ocevpv/FgwVAyAcu3s94BdaZ4Q0e4TwFTg65n5ZGtnRLwyIl7ZcZwnaG7smwp8vOM4J5fjf9fLYyZGr8a57D+eZqx/AcxyTDcfPfz7fEpm/nXnF3+YqfjHsq/zeegaBz0c53XA/ygvP9P+YSQi9gROoHlE9NIefwvahB7+zL6hbGdHxGvb34iIvYDZNNfZL+td7zVWIuKPyjjv3r5/NP+/jDdX1B4cH6RZ2v2LEXEocBewL82aEncDH+1of1fZTunYfybNwmenlR9W/0JzM9iRNNfcd/6PrvFVPc4RcTDN9bnPo/nNyHsjoqOMf8/Mz/e89xquXv191uatV+P8KeBQmqcC7RkR19P89voYYBtgQWbeOxbfgDapeowz818i4hKaBSxvjojvAP9K8+HzKGAr4POZeecYfh/aiHJv4lHl5YvLdr+IuLT896OZeXr575fSjHNrDNuN9P+XceVMxYAoCXdv4FKa/wEXALsDXwT2G+6iOKXdfqXuFeU4+9L8dvN15TyaID0a55fxh58Nc2lW6uz8+lBPO64R6dXfZ23eevhzey1NqPgEzVOB5gFvo/lw8n9l5vk977yGpYd/l99HEyp+ArypHOeNwI+Bd2Tmqb3tuUZoL5qnrR1PMz4Au7Xtmz2cg2zuP/unrF/v0z8lSZIkjZ4zFZIkSZKqGCokSZIkVTFUSJIkSapiqJAkSZJUxVAhSZIkqYqhQpIkSVIVQ4UkSZKkKoYKSZIkSVUMFZIkSZKqGCokSZIkVTFUSJIkSapiqJAkSZJUxVAhSZIkqYqhQpIkSVIVQ4UkSZKkKoYKSZIkSVUMFZIkSZKq/P97u4eRJa5/vwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa777a612b0>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 248,
       "width": 394
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "submission[target_col].hist(bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(SUBMISSIONS/'RA04-26-neuralnet_alltrain_2018_SVD_diffscounts.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
